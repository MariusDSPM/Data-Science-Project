{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to test contextual prompting (i.e. coherent questions)\n",
    "\n",
    "In the example down below, we can refer to \"it\" in the second prompt and the model knows what we are inquiring about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_prompts(prompt1, prompt2, model, max_tokens = 10, temperature = 1):\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # First prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt1})\n",
    "    query1 = client.chat.completions.create(\n",
    "        model = model,\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temperature,\n",
    "        messages = messages\n",
    "    )\n",
    "    reply1 = query1.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply1})\n",
    "\n",
    "    # Second prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt2})\n",
    "    query2 = client.chat.completions.create(\n",
    "        model = model,\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temperature,\n",
    "        messages=messages\n",
    "    )\n",
    "    reply2 = query2.choices[0].message.content\n",
    "\n",
    "    # Print the responses\n",
    "    print(f\"Response to prompt 1: {reply1}\")\n",
    "    print(f\"Response to prompt 2: {reply2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up test prompts\n",
    "prompt_1 = \"Where is the Eiffel Tower located?\"\n",
    "prompt_2 = \"How tall is it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to prompt 1: The Eiffel Tower is located in Paris, France.\n",
      "Response to prompt 2: The Eiffel Tower is approximately 330 meters (1,083 feet\n"
     ]
    }
   ],
   "source": [
    "two_prompts(prompt_1, prompt_2, \"gpt-3.5-turbo\", max_tokens = 15, temperature = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
