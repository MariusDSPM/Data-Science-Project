{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Utility Theory 2\n",
    "\n",
    "This notebook aims to recreate some of the empirical findings of Thaler, R. (1985). Mental accounting and consumer choice. Marketing Science, 4(3), 199-214. \n",
    "Specifically we are interested in whether LLMs' responses are similar to the original responses in **section 3** of the paper, regarding the Beer question.\n",
    "\n",
    "However, we will extend this experiment with the notion of income sensitivity. The various income values are inspired by Brand, James and Israeli, Ayelet and Ngwe, Donald, Using GPT for Market Research (March 21, 2023). Harvard Business School Marketing Unit Working Paper No. 23-062, Available at SSRN: https://ssrn.com/abstract=4395751 or http://dx.doi.org/10.2139/ssrn.4395751. Here, they research the willingness to pay for various goods, prompting the LLM with a stated annual income of $50k, $70k, and $120k.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In order to run this notebook an openAI API key, as well as Replicate API token are required.  \n",
    "Further they have to be set as environment variables named OPENAI_API_KEY and REPLICATE_API_TOKEN respectively.\n",
    "\n",
    "Throughout the process of this project, adjustments have been made to this notebook, therefore the original outputs are no longer included.   \n",
    "For test purposes, the notebook was re-run with a very low number of iterations per prompt and every export of prompts, dictionaries or results was commented out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import replicate\n",
    "from ast import literal_eval\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get openAI API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set plots to be displayed in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up prompts for the experiment\n",
    "\n",
    "- LLMs used in the experiment:\n",
    "    - GPT-3.5-Turbo         (ID = 1)\n",
    "    - GPT-4-1106-Preview    (ID = 2)\n",
    "    - LLama-70b             (ID = 3)\n",
    "\n",
    "We can differentiate between the following scenario combinations:\n",
    "\n",
    "- Place of purchase:\n",
    "    - A fancy resort hotel              (ID = 1)\n",
    "    - A small, run-down grocery store   (ID = 2)\n",
    "\n",
    "- Level of annual income:\n",
    "    - No information about income       (ID = 1)\n",
    "    - $50.000                           (ID = 2)\n",
    "    - $70.000                           (ID = 3)\n",
    "    - $120.000                          (ID = 4)\n",
    "\n",
    "Similar to the Prospect Theory and Decoy Effect notebooks, we will use experiment IDs to run the study. The IDs will be constructed as:\n",
    "\n",
    "*TU2_model__placeofpurchase_income*\n",
    "\n",
    "Therefore, TU2_3_2_1 would mean we used LLama-70b, the beer is to be purchased at a fancy resort hotel and no information about the annual income is given in the prompt.\n",
    "\n",
    "**Median answers: $2.65 for hotel and $1.50 for store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of place of purchase\n",
    "place = [\"a fancy resort hotel.\", \"a small, run-down grocery store.\"]\n",
    "\n",
    "# Set up list of annual income levels\n",
    "incomes = [\"\", \"You have an annual income of $50k.\", \"You have an annual income of $70k.\", \"You have an annual income of $120k.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for resort hotel\n",
    "hotel_prompts = []\n",
    "for income in incomes:\n",
    "    prompt = f\"\"\"You are lying on the beach on a hot day. All you have to drink is ice water. For the last hour you have been thinking about how much you would enjoy a nice cold bottle \n",
    "    of beer. A companion gets up to go make a phone call and offers o bring back a beer from the only nearby place where beer is sold, a fancy resort hotel. He says that the beer might\n",
    "    be expensive and so asks how much you are willing to pay for the beer. He says that he will buy the beer if it costs as much or less than the price you state. But if it costs more \n",
    "    than the price you state he will not buy it. You trust your friend, and there is no possibility of bargaining with the bartender. {income} What price do you tell him?\"\"\"\n",
    "    hotel_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for grocery store \n",
    "store_prompts = []\n",
    "for income in incomes:\n",
    "    prompt = f\"\"\"You are lying on the beach on a hot day. All you have to drink is ice water. For the last hour you have been thinking about how much you would enjoy a nice cold bottle \n",
    "    of beer. A companion gets up to go make a phone call and offers o bring back a beer from the only nearby place where beer is sold, a small, run-down grocery store. He says that\n",
    "    the beer might be expensive and so asks how much you are willing to pay for the beer. He says that he will buy the beer if it costs as much or less than the price you state. \n",
    "    But if it costs more than the price you state he will not buy it. You trust your friend, and there is no possibility of bargaining with the store owner. {income} What price do you tell him?\"\"\"\n",
    "    store_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into one list\n",
    "TU2_prompts = hotel_prompts + store_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TU2_prompts[0]: fancy resort hotel, no info about income (Configuration 1)\n",
    "- TU2_prompts[1]: fancy resort hotel, $50k annual income (Configuration 2)\n",
    "- TU2_prompts[2]: fancy resort hotel, $70k annual income (Configuration 3)\n",
    "- TU2_prompts[3]: fancy resort hotel, $120k annual income (Configuration 4)\n",
    "- TU2_prompts[4]: store, no info about income (Configuration 5)\n",
    "- TU2_prompts[5]: store, $50k annual income (Configuration 6)\n",
    "- TU2_prompts[6]: store, $70k annual income (Configuration 7)\n",
    "- TU2_prompts[7]: store, $120k annual income (Configuration 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prompts for use in Dashboard\n",
    "#with open(\"Dashboard/src/data/Input/TU2_prompts.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(TU2_prompts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up instructions the model should abide by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Answer by only giving a single price in dollars and cents without an explanation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries to store and extract information about the various experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to look up prompt for a given experiment id. key: experiment id, value: prompt\n",
    "TU2_experiment_prompts_dict = {\n",
    "    \"TU2_1_1_1\": TU2_prompts[0],\n",
    "    \"TU2_1_1_2\": TU2_prompts[1],\n",
    "    \"TU2_1_1_3\": TU2_prompts[2],\n",
    "    \"TU2_1_1_4\": TU2_prompts[3],\n",
    "    \"TU2_1_2_1\": TU2_prompts[4],\n",
    "    \"TU2_1_2_2\": TU2_prompts[5],\n",
    "    \"TU2_1_2_3\": TU2_prompts[6],\n",
    "    \"TU2_1_2_4\": TU2_prompts[7],\n",
    "    \"TU2_2_1_1\": TU2_prompts[0],\n",
    "    \"TU2_2_1_2\": TU2_prompts[1],\n",
    "    \"TU2_2_1_3\": TU2_prompts[2],\n",
    "    \"TU2_2_1_4\": TU2_prompts[3],\n",
    "    \"TU2_2_2_1\": TU2_prompts[4],\n",
    "    \"TU2_2_2_2\": TU2_prompts[5],\n",
    "    \"TU2_2_2_3\": TU2_prompts[6],\n",
    "    \"TU2_2_2_4\": TU2_prompts[7],\n",
    "    \"TU2_3_1_1\": TU2_prompts[0],\n",
    "    \"TU2_3_1_2\": TU2_prompts[1],\n",
    "    \"TU2_3_1_3\": TU2_prompts[2],\n",
    "    \"TU2_3_1_4\": TU2_prompts[3],\n",
    "    \"TU2_3_2_1\": TU2_prompts[4],\n",
    "    \"TU2_3_2_2\": TU2_prompts[5],\n",
    "    \"TU2_3_2_3\": TU2_prompts[6],\n",
    "    \"TU2_3_2_4\": TU2_prompts[7]\n",
    "}\n",
    "\n",
    "# Dictionary to look up which model to use for a given experiment id. key: experiment id, value: model name\n",
    "TU2_model_dict = {\n",
    "    \"TU2_1_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_3\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_4\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_2_1_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_3\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_4\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_3_1_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what prompt variable was used in a given experiment. key: experiment id, value: prompt variable\n",
    "TU2_prompt_ids_dict = {\n",
    "    \"TU2_1_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_1_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_1_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_1_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_1_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_1_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_1_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_1_2_4\": \"TU2_prompts[7]\",\n",
    "    \"TU2_2_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_2_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_2_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_2_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_2_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_2_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_2_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_2_2_4\": \"TU2_prompts[7]\",\n",
    "    \"TU2_3_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_3_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_3_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_3_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_3_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_3_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_3_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_3_2_4\": \"TU2_prompts[7]\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what place variable was used in a given experiment. key: experiment id, value: place variable\n",
    "TU2_places_dict = {\n",
    "    \"TU2_1_1_1\": \"hotel\",\n",
    "    \"TU2_1_1_2\": \"hotel\",\n",
    "    \"TU2_1_1_3\": \"hotel\",\n",
    "    \"TU2_1_1_4\": \"hotel\",\n",
    "    \"TU2_1_2_1\": \"grocery\",\n",
    "    \"TU2_1_2_2\": \"grocery\",\n",
    "    \"TU2_1_2_3\": \"grocery\",\n",
    "    \"TU2_1_2_4\": \"grocery\",\n",
    "    \"TU2_2_1_1\": \"hotel\",\n",
    "    \"TU2_2_1_2\": \"hotel\",\n",
    "    \"TU2_2_1_3\": \"hotel\",\n",
    "    \"TU2_2_1_4\": \"hotel\",\n",
    "    \"TU2_2_2_1\": \"grocery\",\n",
    "    \"TU2_2_2_2\": \"grocery\",\n",
    "    \"TU2_2_2_3\": \"grocery\",\n",
    "    \"TU2_2_2_4\": \"grocery\",\n",
    "    \"TU2_3_1_1\": \"hotel\",\n",
    "    \"TU2_3_1_2\": \"hotel\",\n",
    "    \"TU2_3_1_3\": \"hotel\",\n",
    "    \"TU2_3_1_4\": \"hotel\",\n",
    "    \"TU2_3_2_1\": \"grocery\",\n",
    "    \"TU2_3_2_2\": \"grocery\",\n",
    "    \"TU2_3_2_3\": \"grocery\",\n",
    "    \"TU2_3_2_4\": \"grocery\"\n",
    "}   \n",
    "\n",
    "# Dictionary to look up what income variable was used in a given experiment. key: experiment id, value: income variable\n",
    "TU2_income_dict = {\n",
    "    \"TU2_1_1_1\": \"0\",\n",
    "    \"TU2_1_1_2\": \"$50k\",\n",
    "    \"TU2_1_1_3\": \"$70k\",\n",
    "    \"TU2_1_1_4\": \"$120k\",\n",
    "    \"TU2_1_2_1\": \"0\",\n",
    "    \"TU2_1_2_2\": \"$50k\",\n",
    "    \"TU2_1_2_3\": \"$70k\",\n",
    "    \"TU2_1_2_4\": \"$120k\",\n",
    "    \"TU2_2_1_1\": \"0\",\n",
    "    \"TU2_2_1_2\": \"$50k\",\n",
    "    \"TU2_2_1_3\": \"$70k\",\n",
    "    \"TU2_2_1_4\": \"$120k\",\n",
    "    \"TU2_2_2_1\": \"0\",\n",
    "    \"TU2_2_2_2\": \"$50k\",\n",
    "    \"TU2_2_2_3\": \"$70k\",\n",
    "    \"TU2_2_2_4\": \"$120k\",\n",
    "    \"TU2_3_1_1\": \"0\",\n",
    "    \"TU2_3_1_2\": \"$50k\",\n",
    "    \"TU2_3_1_3\": \"$70k\",\n",
    "    \"TU2_3_1_4\": \"$120k\",\n",
    "    \"TU2_3_2_1\": \"0\",\n",
    "    \"TU2_3_2_2\": \"$50k\",\n",
    "    \"TU2_3_2_3\": \"$70k\",\n",
    "    \"TU2_3_2_4\": \"$120k\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what configuration was used in a given experiment. key: experiment id, value: configuration\n",
    "TU2_configuration_dict = {\n",
    "    \"TU2_1_1_1\": 1,\n",
    "    \"TU2_1_1_2\": 2,\n",
    "    \"TU2_1_1_3\": 3,\n",
    "    \"TU2_1_1_4\": 4,\n",
    "    \"TU2_1_2_1\": 5,\n",
    "    \"TU2_1_2_2\": 6,\n",
    "    \"TU2_1_2_3\": 7,\n",
    "    \"TU2_1_2_4\": 8,\n",
    "    \"TU2_2_1_1\": 1,\n",
    "    \"TU2_2_1_2\": 2,\n",
    "    \"TU2_2_1_3\": 3,\n",
    "    \"TU2_2_1_4\": 4,\n",
    "    \"TU2_2_2_1\": 5,\n",
    "    \"TU2_2_2_2\": 6,\n",
    "    \"TU2_2_2_3\": 7,\n",
    "    \"TU2_2_2_4\": 8,\n",
    "    \"TU2_3_1_1\": 1,\n",
    "    \"TU2_3_1_2\": 2,\n",
    "    \"TU2_3_1_3\": 3,\n",
    "    \"TU2_3_1_4\": 4,\n",
    "    \"TU2_3_2_1\": 5,\n",
    "    \"TU2_3_2_2\": 6,\n",
    "    \"TU2_3_2_3\": 7,\n",
    "    \"TU2_3_2_4\": 8 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dictionaries and save for use in Dashboard\n",
    "TU2_dictionaries = [TU2_experiment_prompts_dict, TU2_model_dict, TU2_prompt_ids_dict, TU2_places_dict, TU2_income_dict, TU2_configuration_dict]\n",
    "#with open(\"Dashboard/src/data/Input/TU2_dictionaries.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(TU2_dictionaries, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up functions to repeatedly prompt the LLMs\n",
    "\n",
    "- Helper function to extract dollar amount of given answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dollar_amounts(answers):\n",
    "    # Only return values that start with \"$\"\n",
    "    valid_prices = [item for item in answers if item.startswith(\"$\") and item[1:].replace(',', '').replace('.', '').isdigit()] # check if everything after $ is a digit, exlcuding commas\n",
    "    # Delete the \"$\" from the beginning of each price\n",
    "    prices = [item.replace('$', '') for item in valid_prices]\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times for OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment(experiment_id, n, progress_bar, temperature):\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = TU2_model_dict[experiment_id], \n",
    "            max_tokens = 2,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer by only giving a single price in dollars and cents without an explanation.\"},        \n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"}\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], answers, n_observations, TU2_configuration_dict[experiment_id]]\n",
    "\n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment_dashboard(experiment_id, n, temperature):\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = TU2_model_dict[experiment_id], \n",
    "            max_tokens = 2,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer by only giving a single price in dollars and cents without an explanation.\"},        \n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"}\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "\n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], f\"{answers}\", n_observations, TU2_configuration_dict[experiment_id]]\n",
    "    results = pd.DataFrame(results, index = [\"Experiment_id\", \"Temperature\", \"Model\", \"Place\", \"Income\", \"Answers\", \"Obs.\", \"Configuration\"]).T\n",
    "\n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment_llama(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            TU2_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\":  \"Answer by only giving a single price in dollars and cents without an explanation.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 10, \n",
    "                \"prompt\": f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], answers, n_observations, TU2_configuration_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment_llama_dashboard(experiment_id, n, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            TU2_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\":  \"Answer by only giving a single price in dollars and cents without an explanation.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 10, \n",
    "                \"prompt\": f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "   # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], f\"{answers}\", n_observations, TU2_configuration_dict[experiment_id]]\n",
    "    results = pd.DataFrame(results, index = [\"Experiment_id\", \"Temperature\", \"Model\", \"Place\", \"Income\", \"Answers\", \"Obs.\", \"Configuration\"]).T\n",
    "    \n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to loop run_experiment() over a list of temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_temperature_loop(function, experiment_id, temperature_list = [0.5, 1, 1.5], n = 50):\n",
    "    \"\"\"\n",
    "    Function to run an experiment with different temperature values.\n",
    "    \n",
    "    Args:\n",
    "        function (function): Function to be used for querying ChatGPT i.e. run_experiment()\n",
    "        experiment_id (str): ID of th e experiment to be run. Contains info about prompt and model\n",
    "        temperature_list (list): List of temperature values to be looped over\n",
    "        n: Number of requests for each prompt per temperature value\n",
    "        max_tokens: Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results_df: Dataframe with experiment results\n",
    "        probs_df: Dataframe with answer probabilities\n",
    "    \"\"\"    \n",
    "    # Empty list for storing results\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize progress bar -> used as input for run_experiment()\n",
    "    progress_bar = tqdm(range(n*len(temperature_list)))\n",
    "\n",
    "    # Loop over different temperature values, calling the input function n times each (i.e. queriyng ChatGPT n times)\n",
    "    for temperature in temperature_list:\n",
    "        results = function(experiment_id = experiment_id, n = n, temperature = temperature, progress_bar = progress_bar) \n",
    "        results_list.append(results)\n",
    "       \n",
    "\n",
    "    # Horizontally concatenate the results, transpose, and set index\n",
    "    results_df = pd.DataFrame(results_list).transpose().set_index(pd.Index(\n",
    "        [\"experiment_id\", \"temperature\", \"model\", \"place\", \"income\", \"answers\", \"obs.\", \"configuration\"]))\n",
    "  \n",
    "   \n",
    "    # Return some information about the experiment as a check\n",
    "    check = f\"In this run, a total of {n*len(temperature_list)} requests were made using {TU2_prompt_ids_dict[experiment_id]}.\"\n",
    "    # Print information about the experiment\n",
    "    print(check)\n",
    " \n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temperature values to be looped over\n",
    "temperature_list = [0.01, 0.5, 1, 1.5, 2] # 0.01 for llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPT-3.5-turbo we issue 100 requests per prompt & temperature value\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "results_1_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_1\", temperature_list = temperature_list , n = N)\n",
    "results_1.append(results_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2 = []\n",
    "results_2_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_2\", temperature_list = temperature_list, n = N)\n",
    "results_2.append(results_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3 = []\n",
    "results_3_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_3\", temperature_list = temperature_list, n = N)\n",
    "results_3.append(results_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4 = []\n",
    "results_4_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_4\", temperature_list = temperature_list, n = N)\n",
    "results_4.append(results_4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5 = []\n",
    "results_5_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_1\", temperature_list = temperature_list, n = N)\n",
    "results_5.append(results_5_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6 = []\n",
    "results_6_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_2\", temperature_list = temperature_list, n = N)\n",
    "results_6.append(results_6_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7 = []\n",
    "results_7_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_3\", temperature_list = temperature_list, n = N)\n",
    "results_7.append(results_7_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8 = []\n",
    "results_8_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_4\", temperature_list = temperature_list, n = N)\n",
    "results_8.append(results_8_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: GPT-4-1106-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of requests per temperature value\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_1\", temperature_list = temperature_list, n = N)\n",
    "results_1.append(results_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:50<00:00, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_2\", temperature_list = temperature_list, n = N)\n",
    "results_2.append(results_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_3\", temperature_list = temperature_list, n = N)\n",
    "results_3.append(results_3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_4\", temperature_list = temperature_list, n = N)\n",
    "results_4.append(results_4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_1\", temperature_list = temperature_list, n = N)\n",
    "results_5.append(results_5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_2\", temperature_list = temperature_list, n = N)\n",
    "results_6.append(results_6_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_3\", temperature_list = temperature_list, n = N)\n",
    "results_7.append(results_7_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_4\", temperature_list = temperature_list, n = N)\n",
    "results_8.append(results_8_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: LLama-2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of requests per temperature value\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_1\", temperature_list = temperature_list, n = N)\n",
    "results_1.append(results_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_2\", temperature_list = temperature_list, n = N)\n",
    "results_2.append(results_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_3\", temperature_list = temperature_list, n = N)\n",
    "results_3.append(results_3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_4\", temperature_list = temperature_list, n = N)\n",
    "results_4.append(results_4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_1\", temperature_list = temperature_list, n = N)\n",
    "results_5.append(results_5_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_2\", temperature_list = temperature_list, n = N)\n",
    "results_6.append(results_6_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_3\", temperature_list = temperature_list, n = N)\n",
    "results_7.append(results_7_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 10 requests were made using TU2_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_4\", temperature_list = temperature_list, n = N)\n",
    "results_8.append(results_8_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all results and save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_id</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Model</th>\n",
       "      <th>Place</th>\n",
       "      <th>Income</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Configuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>[$10, $10]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>[$10, $5]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>[$10, $5]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>[$5, $5]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>[$5, $8]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>[$8.50, $8.50]</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>[$8.50, $8.50]</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>[$7.50, $5.99]</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>[$8.50, $7.50]</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>[Sure! The price I would give my companion, $7...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experiment_id Temperature          Model    Place Income  \\\n",
       "0      TU2_1_1_1        0.01  gpt-3.5-turbo    hotel      0   \n",
       "1      TU2_1_1_1         0.5  gpt-3.5-turbo    hotel      0   \n",
       "2      TU2_1_1_1         1.0  gpt-3.5-turbo    hotel      0   \n",
       "3      TU2_1_1_1         1.5  gpt-3.5-turbo    hotel      0   \n",
       "4      TU2_1_1_1         2.0  gpt-3.5-turbo    hotel      0   \n",
       "..           ...         ...            ...      ...    ...   \n",
       "0      TU2_3_2_4        0.01    llama-2-70b  grocery  $120k   \n",
       "1      TU2_3_2_4         0.5    llama-2-70b  grocery  $120k   \n",
       "2      TU2_3_2_4         1.0    llama-2-70b  grocery  $120k   \n",
       "3      TU2_3_2_4         1.5    llama-2-70b  grocery  $120k   \n",
       "4      TU2_3_2_4         2.0    llama-2-70b  grocery  $120k   \n",
       "\n",
       "                                              Answers Obs. Configuration  \n",
       "0                                          [$10, $10]    2             1  \n",
       "1                                           [$10, $5]    2             1  \n",
       "2                                           [$10, $5]    2             1  \n",
       "3                                            [$5, $5]    2             1  \n",
       "4                                            [$5, $8]    2             1  \n",
       "..                                                ...  ...           ...  \n",
       "0                                      [$8.50, $8.50]    2             8  \n",
       "1                                      [$8.50, $8.50]    2             8  \n",
       "2                                      [$7.50, $5.99]    2             8  \n",
       "3                                      [$8.50, $7.50]    2             8  \n",
       "4   [Sure! The price I would give my companion, $7...    1             8  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate results\n",
    "results_1_df = pd.concat(results_1, axis = 1).transpose()\n",
    "results_2_df = pd.concat(results_2, axis = 1).transpose()\n",
    "results_3_df = pd.concat(results_3, axis = 1).transpose()\n",
    "results_4_df = pd.concat(results_4, axis = 1).transpose()\n",
    "results_5_df = pd.concat(results_5, axis = 1).transpose()\n",
    "results_6_df = pd.concat(results_6, axis = 1).transpose()\n",
    "results_7_df = pd.concat(results_7, axis = 1).transpose()\n",
    "results_8_df = pd.concat(results_8, axis = 1).transpose()\n",
    "\n",
    "# Concatenate all results\n",
    "TU2_results = pd.concat([results_1_df, results_2_df, results_3_df, results_4_df, \n",
    "                         results_5_df, results_6_df, results_7_df, results_8_df], axis = 0)\n",
    "\n",
    "# Rename LLama model\n",
    "TU2_results['model'] = TU2_results['model'].replace('meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', \n",
    "                                  'llama-2-70b')\n",
    "\n",
    "\n",
    "# Capitalize first letter of column names\n",
    "TU2_results.columns = TU2_results.columns.str.capitalize()\n",
    "\n",
    "# Display results\n",
    "TU2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to .csv\n",
    "# TU2_results.to_csv(\"Dashboard/src/data/Output/TU2_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_id</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Model</th>\n",
       "      <th>Place</th>\n",
       "      <th>Income</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Configuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$10', '$10', '$10', '$10', '$10', '$10', '$1...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$10', '$10', '$10', '$10', '$5', '$10', '$5'...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$5', '$10', '$5', '$10', '$5', '$10', '$10',...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$5', '$10', '$10', '$5', '$5', '$10', '$10',...</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TU2_1_1_1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$10', '$10', '$5', '$5', '$10', '$5', '$10',...</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>['$8.50', '$8.50', '$8.50', '$8.50', '$8.50', ...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>['$7.50', '$7.50', '$8.50', '$8.50', '$8.50', ...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>['$8.50', '$8.50', '$5.50', '$7.50', '$8.50', ...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>1.50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>['$7.50', '$7.50', '$7.50', '$5.50', '$8.50', ...</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>TU2_3_2_4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>grocery</td>\n",
       "      <td>$120k</td>\n",
       "      <td>['$4.50', \"Sure! Here's my answer:\", '$5.50', ...</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Experiment_id  Temperature          Model    Place Income  \\\n",
       "0       TU2_1_1_1         0.01  gpt-3.5-turbo    hotel      0   \n",
       "1       TU2_1_1_1         0.50  gpt-3.5-turbo    hotel      0   \n",
       "2       TU2_1_1_1         1.00  gpt-3.5-turbo    hotel      0   \n",
       "3       TU2_1_1_1         1.50  gpt-3.5-turbo    hotel      0   \n",
       "4       TU2_1_1_1         2.00  gpt-3.5-turbo    hotel      0   \n",
       "..            ...          ...            ...      ...    ...   \n",
       "115     TU2_3_2_4         0.01    llama-2-70b  grocery  $120k   \n",
       "116     TU2_3_2_4         0.50    llama-2-70b  grocery  $120k   \n",
       "117     TU2_3_2_4         1.00    llama-2-70b  grocery  $120k   \n",
       "118     TU2_3_2_4         1.50    llama-2-70b  grocery  $120k   \n",
       "119     TU2_3_2_4         2.00    llama-2-70b  grocery  $120k   \n",
       "\n",
       "                                               Answers  Obs.  Configuration  \n",
       "0    ['$10', '$10', '$10', '$10', '$10', '$10', '$1...   100              1  \n",
       "1    ['$10', '$10', '$10', '$10', '$5', '$10', '$5'...   100              1  \n",
       "2    ['$5', '$10', '$5', '$10', '$5', '$10', '$10',...   100              1  \n",
       "3    ['$5', '$10', '$10', '$5', '$5', '$10', '$10',...    98              1  \n",
       "4    ['$10', '$10', '$5', '$5', '$10', '$5', '$10',...    84              1  \n",
       "..                                                 ...   ...            ...  \n",
       "115  ['$8.50', '$8.50', '$8.50', '$8.50', '$8.50', ...    50              8  \n",
       "116  ['$7.50', '$7.50', '$8.50', '$8.50', '$8.50', ...    50              8  \n",
       "117  ['$8.50', '$8.50', '$5.50', '$7.50', '$8.50', ...    50              8  \n",
       "118  ['$7.50', '$7.50', '$7.50', '$5.50', '$8.50', ...    50              8  \n",
       "119  ['$4.50', \"Sure! Here's my answer:\", '$5.50', ...    27              8  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU2_results = pd.read_csv('Dashboard/src/data/Output/TU2_results.csv')\n",
    "TU2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_plot_results_old(df):\n",
    "    \n",
    "    # Transpose for plotting\n",
    "    df = df.transpose()\n",
    "    # Get temperature value \n",
    "    temperature = df.loc[\"Temperature\"].iloc[0]\n",
    "    # Get model name\n",
    "    model = df.loc[\"Model\"].iloc[0]\n",
    "    # Get number of observations \n",
    "    n_observations = df.loc[\"Obs.\"].iloc[0] \n",
    "    # Get place\n",
    "    place = df.loc[\"Place\"].iloc[0]\n",
    "    # Adjust name of place for plot title\n",
    "    if place == \"grocery\":\n",
    "        place = \"grocery store\"\n",
    "\n",
    "    # Apply literal_eval to work with list of strings\n",
    "    answers = df.loc[\"Answers\"].apply(literal_eval).iloc[0]\n",
    "    # Get stated WTP\n",
    "    prices = extract_dollar_amounts(answers)\n",
    "    # Convert to float\n",
    "    prices = [float(price) for price in prices]\n",
    "    # Get max, mean and median\n",
    "    median = np.median(prices)\n",
    "    mean = np.mean(prices)\n",
    "    max = np.max(prices)\n",
    "\n",
    "    # Adjust prices so that every value above 30 is set to 30, deals with outliers\n",
    "    prices = [30.00 if price > 30 else price for price in prices]\n",
    "\n",
    "    # Create the histogram using custom bins\n",
    "    fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x = list(Counter(prices).keys()),\n",
    "        y = list(Counter(prices).values()),\n",
    "        name=\"Model answers\",\n",
    "        customdata=[n_observations] * len(prices),\n",
    "        hovertemplate=\"Value: %{x}<br>Number of observations: %{y}<br>Number of total observations: %{customdata}<extra></extra>\",\n",
    "        marker_color=\"rgb(55, 83, 109)\",\n",
    "        width=0.4 ,  # Adjust the width of the bars if needed\n",
    "    ),\n",
    "    # Add vertical line for median\n",
    "    go.Scatter(\n",
    "        x = [median, median], #start and enf of x\n",
    "        y = [0, Counter(prices).most_common(1)[0][1]], # count of most common price\n",
    "        mode=\"lines\",\n",
    "        name=\"Median\",\n",
    "        line=dict(color=\"red\", width=4, dash=\"dash\"),\n",
    "        hovertemplate = \"Median: %{x}<extra></extra>\",\n",
    "),\n",
    "    # Add vertical line for mean\n",
    "    go.Scatter(\n",
    "        x = [mean, mean], #start and enf of x\n",
    "        y = [0, Counter(prices).most_common(1)[0][1]], # count of most common price\n",
    "        mode=\"lines\",\n",
    "        name=\"Mean\",\n",
    "        line=dict(color=\"green\", width=4, dash=\"dash\"),\n",
    "        hovertemplate = \"Mean: %{x}<extra></extra>\",\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        title = \"Willingness to pay (USD)\",\n",
    "        titlefont_size = 18,\n",
    "        tickfont_size = 16,\n",
    "        tickformat=\".2f\",\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = \"Frequency\",\n",
    "        titlefont_size = 18,\n",
    "        tickfont_size = 16,\n",
    "    ),\n",
    "    title = dict(\n",
    "    text =  f\"Distribution of {model}'s WTP for beer at the {place} for temperature {temperature}\",\n",
    "    x = 0.5, \n",
    "    y = 0.95,\n",
    "    font_size = 18,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=1.01, \n",
    "        y=0.9,\n",
    "        font=dict(family='Arial', size=12, color='black'),\n",
    "        bordercolor='black',  \n",
    "        borderwidth=2,  \n",
    "    ),\n",
    "    showlegend = True,\n",
    "    width = 1000,\n",
    "    margin=dict(t=60)\n",
    "    )\n",
    "    # Adjust x-axis labels to show 30+ to symbolize aggregation\n",
    "    fig.update_xaxes(\n",
    "    tickvals = sorted(fig.data[0].x),\n",
    "    ticktext=[\"$30+\" if tick_value == 30.0 else tick_value for tick_value in sorted(set(fig.data[0].x))],\n",
    ")\n",
    "\n",
    "    print(f\"The maximum WTP stated by {model} for beer at the {place} for temperature {temperature} is ${max}.\")\n",
    "    # Show the plot\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TU2_results\n",
    "model = \"llama-2-70b\"\n",
    "temperature = 2\n",
    "place = \"hotel\"\n",
    "income = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_id</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Model</th>\n",
       "      <th>Place</th>\n",
       "      <th>Income</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Configuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TU2_3_1_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>['$4.50', '$6.50', '$7.50', '$5.00', '$5.50', ...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experiment_id  Temperature        Model  Place Income  \\\n",
       "14     TU2_3_1_1          2.0  llama-2-70b  hotel      0   \n",
       "\n",
       "                                              Answers  Obs.  Configuration  \n",
       "14  ['$4.50', '$6.50', '$7.50', '$5.00', '$5.50', ...    50              1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = TU2_results[(TU2_results[\"Model\"] == model) & (TU2_results[\"Temperature\"] == temperature) & (TU2_results[\"Place\"] == place) & (TU2_results[\"Income\"] == income)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_plot_results(df):\n",
    "\n",
    "\n",
    "    # Transpose for plotting\n",
    "    df = df.transpose()\n",
    "    # Get model name\n",
    "    model = df.loc[\"Model\"].iloc[0]\n",
    "    if model == \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\":\n",
    "        model = \"llama-2-70b\"\n",
    "    # Get temperature value \n",
    "    temperature = df.loc[\"Temperature\"].iloc[0]\n",
    "    # Get number of observations \n",
    "    n_observations = df.loc[\"Obs.\"].iloc[0] \n",
    "    # Get place\n",
    "    place = df.loc[\"Place\"].iloc[0]\n",
    "    # Adjust name of place for plot title\n",
    "    if place == \"grocery\":\n",
    "        place = \"Grocery store\"\n",
    "    if place == \"hotel\":\n",
    "        place = \"Hotel\"\n",
    "    # Get income\n",
    "    income = df.loc[\"Income\"].iloc[0]\n",
    "    if income == \"0\":\n",
    "        income = \"No information\"\n",
    "    # Apply literal_eval to work with list of strings\n",
    "    answers = df.loc[\"Answers\"].apply(literal_eval).iloc[0]\n",
    "    # Get stated WTP\n",
    "    prices = extract_dollar_amounts(answers)\n",
    "    sorted_prices = sorted(prices, key=lambda x: float(x))\n",
    "    numeric_prices = [float(price) for price in prices]\n",
    "    # Get mean and median\n",
    "    mean = np.round(np.mean(numeric_prices),2).astype(str)\n",
    "    median = np.round(np.median(numeric_prices),2).astype(str)\n",
    "    # Get number of unique answers\n",
    "    num_unique_answers = len(set(prices))\n",
    "   \n",
    "\n",
    "    fig = go.Figure(data = [\n",
    "    go.Histogram(x = sorted_prices,\n",
    "                    customdata = [n_observations] * num_unique_answers,\n",
    "                    hovertemplate = \"Price asked: $%{x} <br>Frequency: %{y}<br>Total answers: %{customdata}<br>Mean: $\" + mean + \"<br>Median: $\" + median +\"<extra></extra>\",\n",
    "                    marker_color = \"rgb(55, 83, 109)\",\n",
    "                    name = \"Place: \" + place + \"<br>Income: \" + income,\n",
    "),\n",
    "    ])\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            title=\"Price asked ($)\",\n",
    "            titlefont_size=18,\n",
    "            tickfont_size=16,\n",
    "            tickformat=\".2f\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Frequency\",\n",
    "            titlefont_size=18,\n",
    "            tickfont_size=16,\n",
    "        ),\n",
    "        title=dict(\n",
    "            text=f\"Distribution of {model}'s WTP for temperature {temperature}\",\n",
    "            x=0.5,\n",
    "            y=0.95,\n",
    "            font_size=18,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=1.01,  \n",
    "            y=0.9,\n",
    "            font=dict(family='Arial', size=16, color='black'),\n",
    "            bordercolor='black',  \n",
    "            borderwidth=2,           \n",
    "        ),\n",
    "        showlegend=True,\n",
    "        width=1000,\n",
    "        margin=dict(t=60),\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Show the plot\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          10
         ],
         "hovertemplate": "Price asked: $%{x} <br>Frequency: %{y}<br>Total answers: %{customdata}<br>Mean: $5.0<br>Median: $5.0<extra></extra>",
         "marker": {
          "color": "rgb(55, 83, 109)"
         },
         "name": "Place: Hotel<br>Income: No information",
         "type": "histogram",
         "x": [
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00",
          "5.00"
         ]
        }
       ],
       "layout": {
        "legend": {
         "bordercolor": "black",
         "borderwidth": 2,
         "font": {
          "color": "black",
          "family": "Arial",
          "size": 16
         },
         "x": 1.01,
         "y": 0.9
        },
        "margin": {
         "t": 60
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 18
         },
         "text": "Distribution of llama-2-70b's WTP for temperature 0.5",
         "x": 0.5,
         "y": 0.95
        },
        "width": 1000,
        "xaxis": {
         "tickfont": {
          "size": 16
         },
         "tickformat": ".2f",
         "title": {
          "font": {
           "size": 18
          },
          "text": "Price asked ($)"
         }
        },
        "yaxis": {
         "tickfont": {
          "size": 16
         },
         "title": {
          "font": {
           "size": 18
          },
          "text": "Frequency"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TU2_plot_results(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
