{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Utility Theory\n",
    "\n",
    "This notebook aims to recreate some of the empirical findings of Thaler, R. (1985). Mental accounting and consumer choice. Marketing Science, 4(3), 199-214. \n",
    "Specifically we are interested in whether LLMs' responses are similar to the original responses in **section 3** of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get openAI API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set plots to be displayed in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up prompts for the experiment\n",
    "\n",
    "\n",
    "\n",
    "- LLMs used in the experiment:\n",
    "    - GPT-3.5-Turbo         (ID = 1)\n",
    "    - GPT-4-1106-Preview    (ID = 2)\n",
    "    - LLama-70b             (ID = 3)\n",
    "\n",
    "We can differentiate between the following scenario combinations:\n",
    "\n",
    "- Initial ticket price:\n",
    "    - free                  (ID = 1)\n",
    "    - $5 (as on ticket)     (ID = 2)\n",
    "    - $10                   (ID = 3)\n",
    "- Current market price:\n",
    "    - $5                    (ID = 1)\n",
    "    - $10                   (ID = 2)\n",
    "- Selling to:\n",
    "    - Friend                (ID = 1)\n",
    "    - Stranger              (ID = 2)\n",
    "\n",
    "\n",
    "\n",
    "Similar to the Prospect Theory and Decoy Effect notebooks, we will use experiment IDs to run the study. The IDs will be constructed as:\n",
    "\n",
    "TU_model_initialprice_currentprice_buyer\n",
    "\n",
    "Therefore, TU_2_2_1_2 would mean we used GPT-4-1106-Preview, an initial ticket price of $5, a current market price of $5 as well and we are selling to a stranger.\n",
    "\n",
    "We leave out the information of the respondent being a student in all prompts. From experience, the more concise a prompt is, the better the answer quality. \n",
    "Since the job status/education level is not of interest here, we leave out this information in order to formulate clearer prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of initial costs\n",
    "initial_costs = [\"but you were given your tickets for free by a friend.\", \"which is what you paid for each ticket.\", \"but you paid $10 each for your tickets when you bought them.\"]\n",
    "\n",
    "# Set up list of current ticket prices\n",
    "orientation_prices = [\"$5\", \"$10\"]\n",
    "\n",
    "# Set up list of potential buyers in a scenario\n",
    "potential_buyers = [\"friend?\", \"stranger?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constructing the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "TU_prompts = []\n",
    "for costs in initial_costs:\n",
    "    for orientation_price in orientation_prices:\n",
    "        for potential_buyer in potential_buyers:\n",
    "            prompt = f\"\"\"Imagine that you are going to a soldout Cornell hockey playoff game, and you have an extra ticket to sell or give away. The price marked on the ticket is $5 {costs}\n",
    "            You get to the game early to make sure you get rid of the ticket. An informal survey of people selling tickets indicates that the going price is {orientation_price}. \n",
    "            You find someone who wants the ticket and takes out his wallet to pay you. He asks, how much you want for the ticket. \n",
    "            Assume that there is now law against charging a price higher than that marked on the ticket. What price do you ask for, if he is a {potential_buyer}\"\"\"\n",
    "            TU_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TU_prompts[0]: free, $5, friend  -> 1_1_1 (Configuration 1)\n",
    "- TU_prompts[1]: free, $5, stranger -> 1_1_2 (Configuration 2)\n",
    "- TU_prompts[2]: free, $10, friend -> 1_2_1 (Configuration 3)\n",
    "- TU_prompts[3]: free, $10, stranger -> 1_2_2 (Configuration 4)\n",
    "- TU_prompts[4]: $5, $5, friend -> 2_1_1 (Configuration 5)\n",
    "- TU_prompts[5]: $5, $5, stranger -> 2_1_2 (Configuration 6)\n",
    "- TU_prompts[6]: $5, $10, friend -> 2_2_1 (Configuration 7)\n",
    "- TU_prompts[7]: $5, $10, stranger -> 2_2_2 (Configuration 8)\n",
    "- TU_prompts[8]: $10, $5, friend -> 3_1_1  (Configuration 9)\n",
    "- TU_prompts[9]: $10, $5, stranger -> 3_1_2 (Configuration 10)\n",
    "- TU_prompts[10]: $10, $10, friend -> 3_2_1 (Configuration 11)\n",
    "- TU_prompts[11]: $10, $10, stranger -> 3_2_2 (Configuration 12)\n",
    "\n",
    "The results of the original experiment are:\n",
    "\n",
    "\n",
    "| Configuration   | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   |\n",
    "|------------------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "| $0              | 68%  | 6%   | 65%  | 6%   | 14%  | 0%   | 7%   | 0%   | 0%   | 0%   | 0%   | 0%   |\n",
    "| $5              | 26%  | 77%  | 26%  | 16%  | 79%  | 79%  | 79%  | 14%  | 69%  | 42%  | 15%  | 0%   |\n",
    "| $10             | 3%   | 10%  | 6%   | 58%  | 0%   | 7%   | 4%   | 57%  | 23%  | 46%  | 69%  | 73%  |\n",
    "| Other          | 6%   | 6%   | 3%   | 19%  | 7%   | 14%  | 9%   | 29%  | 8%   | 12%  | 15%  | 27%  |\n",
    "| N              | 31   | 31   | 31   | 31   | 28   | 28   | 28   | 28   | 26   | 26   | 26   | 26   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_results = {\n",
    "    \"og_results_1\" : [68, 26, 3, 6],\n",
    "    \"og_results_2\" : [6, 77, 10, 6],\n",
    "    \"og_results_3\" : [65, 26, 6, 3],\n",
    "    \"og_results_4\" : [6, 16, 58, 19],\n",
    "    \"og_results_5\" : [14, 79, 7, 14],\n",
    "    \"og_results_6\" : [0, 79, 7, 14],\n",
    "    \"og_results_7\" : [7, 79, 4, 9],\n",
    "    \"og_results_8\" : [0, 14, 57, 29],\n",
    "    \"og_results_9\" : [0, 69, 23, 8],\n",
    "    \"og_results_10\" : [0, 42, 46, 12],\n",
    "    \"og_results_11\" : [0, 15, 69, 15],\n",
    "    \"og_results_12\" : [0, 0, 73, 27]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up instructions the model should abide by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Answer by only giving a single price in dollars and cents without an explanation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionaries to extract information about the different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to look up prompt for a given experiment id. key: experiment id, value: prompt\n",
    "TU_experiment_prompts_dict = {\n",
    "    \"TU_1_1_1_1\": TU_prompts[0],\n",
    "    \"TU_1_1_1_2\": TU_prompts[1],\n",
    "    \"TU_1_1_2_1\": TU_prompts[2],\n",
    "    \"TU_1_1_2_2\": TU_prompts[3],\n",
    "    \"TU_1_2_1_1\": TU_prompts[4],\n",
    "    \"TU_1_2_1_2\": TU_prompts[5],\n",
    "    \"TU_1_2_2_1\": TU_prompts[6],\n",
    "    \"TU_1_2_2_2\": TU_prompts[7],\n",
    "    \"TU_1_3_1_1\": TU_prompts[8],\n",
    "    \"TU_1_3_1_2\": TU_prompts[9],\n",
    "    \"TU_1_3_2_1\": TU_prompts[10],\n",
    "    \"TU_1_3_2_2\": TU_prompts[11],\n",
    "    \"TU_2_1_1_1\": TU_prompts[0],\n",
    "    \"TU_2_1_1_2\": TU_prompts[1],\n",
    "    \"TU_2_1_2_1\": TU_prompts[2],\n",
    "    \"TU_2_1_2_2\": TU_prompts[3],\n",
    "    \"TU_2_2_1_1\": TU_prompts[4],\n",
    "    \"TU_2_2_1_2\": TU_prompts[5],\n",
    "    \"TU_2_2_2_1\": TU_prompts[6],\n",
    "    \"TU_2_2_2_2\": TU_prompts[7],\n",
    "    \"TU_2_3_1_1\": TU_prompts[8],\n",
    "    \"TU_2_3_1_2\": TU_prompts[9],\n",
    "    \"TU_2_3_2_1\": TU_prompts[10],\n",
    "    \"TU_2_3_2_2\": TU_prompts[11],\n",
    "    \"TU_3_1_1_1\": TU_prompts[0],\n",
    "    \"TU_3_1_1_2\": TU_prompts[1],\n",
    "    \"TU_3_1_2_1\": TU_prompts[2],\n",
    "    \"TU_3_1_2_2\": TU_prompts[3],\n",
    "    \"TU_3_2_1_1\": TU_prompts[4],\n",
    "    \"TU_3_2_1_2\": TU_prompts[5],\n",
    "    \"TU_3_2_2_1\": TU_prompts[6],\n",
    "    \"TU_3_2_2_2\": TU_prompts[7],\n",
    "    \"TU_3_3_1_1\": TU_prompts[8],\n",
    "    \"TU_3_3_1_2\": TU_prompts[9],\n",
    "    \"TU_3_3_2_1\": TU_prompts[10],\n",
    "    \"TU_3_3_2_2\": TU_prompts[11],\n",
    "}\n",
    "\n",
    "# Dictionary to look up which model to use for a given experiment id. key: experiment id, value: model name\n",
    "TU_model_dict = {\n",
    "    \"TU_1_1_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_1_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_1_2_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_1_2_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_2_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_2_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_2_2_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_2_2_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_3_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_3_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_3_2_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU_1_3_2_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU_2_1_1_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_1_1_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_1_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_1_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_2_1_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_2_1_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_2_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_2_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_3_1_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_3_1_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_3_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU_2_3_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU_3_1_1_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_1_1_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_1_2_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_1_2_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_2_1_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_2_1_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_2_2_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_2_2_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_3_1_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_3_1_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_3_2_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU_3_3_2_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "}\n",
    "\n",
    "# Dictionary to look up what prompt was used for a given experiment id. key: experiment id, value: prompt variable name\n",
    "TU_prompt_ids_dict = {\n",
    "    \"TU_1_1_1_1\": \"TU_prompts[0]\",\n",
    "    \"TU_1_1_1_2\": \"TU_prompts[1]\",\n",
    "    \"TU_1_1_2_1\": \"TU_prompts[2]\",\n",
    "    \"TU_1_1_2_2\": \"TU_prompts[3]\",\n",
    "    \"TU_1_2_1_1\": \"TU_prompts[4]\",\n",
    "    \"TU_1_2_1_2\": \"TU_prompts[5]\",\n",
    "    \"TU_1_2_2_1\": \"TU_prompts[6]\",\n",
    "    \"TU_1_2_2_2\": \"TU_prompts[7]\",\n",
    "    \"TU_1_3_1_1\": \"TU_prompts[8]\",\n",
    "    \"TU_1_3_1_2\": \"TU_prompts[9]\",\n",
    "    \"TU_1_3_2_1\": \"TU_prompts[10]\",\n",
    "    \"TU_1_3_2_2\": \"TU_prompts[11]\",\n",
    "    \"TU_2_1_1_1\": \"TU_prompts[0]\",\n",
    "    \"TU_2_1_1_2\": \"TU_prompts[1]\",\n",
    "    \"TU_2_1_2_1\": \"TU_prompts[2]\",\n",
    "    \"TU_2_1_2_2\": \"TU_prompts[3]\",\n",
    "    \"TU_2_2_1_1\": \"TU_prompts[4]\",\n",
    "    \"TU_2_2_1_2\": \"TU_prompts[5]\",\n",
    "    \"TU_2_2_2_1\": \"TU_prompts[6]\",\n",
    "    \"TU_2_2_2_2\": \"TU_prompts[7]\",\n",
    "    \"TU_2_3_1_1\": \"TU_prompts[8]\",\n",
    "    \"TU_2_3_1_2\": \"TU_prompts[9]\",\n",
    "    \"TU_2_3_2_1\": \"TU_prompts[10]\",\n",
    "    \"TU_2_3_2_2\": \"TU_prompts[11]\",\n",
    "    \"TU_3_1_1_1\": \"TU_prompts[0]\",\n",
    "    \"TU_3_1_1_2\": \"TU_prompts[1]\",\n",
    "    \"TU_3_1_2_1\": \"TU_prompts[2]\",\n",
    "    \"TU_3_1_2_2\": \"TU_prompts[3]\",\n",
    "    \"TU_3_2_1_1\": \"TU_prompts[4]\",\n",
    "    \"TU_3_2_1_2\": \"TU_prompts[5]\",\n",
    "    \"TU_3_2_2_1\": \"TU_prompts[6]\",\n",
    "    \"TU_3_2_2_2\": \"TU_prompts[7]\",\n",
    "    \"TU_3_3_1_1\": \"TU_prompts[8]\",\n",
    "    \"TU_3_3_1_2\": \"TU_prompts[9]\",\n",
    "    \"TU_3_3_2_1\": \"TU_prompts[10]\",\n",
    "    \"TU_3_3_2_2\": \"TU_prompts[11]\",\n",
    "    }\n",
    "\n",
    "# Dictionary to look up initital ticket cotsts for a given experiment id. key: experiment id, value: initial costs\n",
    "TU_initial_costs_dict = {\n",
    "    \"TU_1_1_1_1\": 0,\n",
    "    \"TU_1_1_1_2\": 0,\n",
    "    \"TU_1_1_2_1\": 0,\n",
    "    \"TU_1_1_2_2\": 0,\n",
    "    \"TU_1_2_1_1\": 5,\n",
    "    \"TU_1_2_1_2\": 5,\n",
    "    \"TU_1_2_2_1\": 5,\n",
    "    \"TU_1_2_2_2\": 5,\n",
    "    \"TU_1_3_1_1\": 10,\n",
    "    \"TU_1_3_1_2\": 10,\n",
    "    \"TU_1_3_2_1\": 10,\n",
    "    \"TU_1_3_2_2\": 10,\n",
    "    \"TU_2_1_1_1\": 0,\n",
    "    \"TU_2_1_1_2\": 0,\n",
    "    \"TU_2_1_2_1\": 0,\n",
    "    \"TU_2_1_2_2\": 0,\n",
    "    \"TU_2_2_1_1\": 5,\n",
    "    \"TU_2_2_1_2\": 5,\n",
    "    \"TU_2_2_2_1\": 5,\n",
    "    \"TU_2_2_2_2\": 5,\n",
    "    \"TU_2_3_1_1\": 10,\n",
    "    \"TU_2_3_1_2\": 10,\n",
    "    \"TU_2_3_2_1\": 10,\n",
    "    \"TU_2_3_2_2\": 10,\n",
    "    \"TU_3_1_1_1\": 0,\n",
    "    \"TU_3_1_1_2\": 0,\n",
    "    \"TU_3_1_2_1\": 0,\n",
    "    \"TU_3_1_2_2\": 0,\n",
    "    \"TU_3_2_1_1\": 5,\n",
    "    \"TU_3_2_1_2\": 5,\n",
    "    \"TU_3_2_2_1\": 5,\n",
    "    \"TU_3_2_2_2\": 5,\n",
    "    \"TU_3_3_1_1\": 10,\n",
    "    \"TU_3_3_1_2\": 10,\n",
    "    \"TU_3_3_2_1\": 10,\n",
    "    \"TU_3_3_2_2\": 10,\n",
    "    }\n",
    "\n",
    "# Dictionary to look up orientation prices for a given experiment id. key: experiment id, value: orientation price\n",
    "TU_orientation_prices_dict = {\n",
    "    \"TU_1_1_1_1\": 5,\n",
    "    \"TU_1_1_1_2\": 5,\n",
    "    \"TU_1_1_2_1\": 10,\n",
    "    \"TU_1_1_2_2\": 10,\n",
    "    \"TU_1_2_1_1\": 5,\n",
    "    \"TU_1_2_1_2\": 5,\n",
    "    \"TU_1_2_2_1\": 10,\n",
    "    \"TU_1_2_2_2\": 10,\n",
    "    \"TU_1_3_1_1\": 5,\n",
    "    \"TU_1_3_1_2\": 5,\n",
    "    \"TU_1_3_2_1\": 10,\n",
    "    \"TU_1_3_2_2\": 10,\n",
    "    \"TU_2_1_1_1\": 5,\n",
    "    \"TU_2_1_1_2\": 5,\n",
    "    \"TU_2_1_2_1\": 10,\n",
    "    \"TU_2_1_2_2\": 10,\n",
    "    \"TU_2_2_1_1\": 5,\n",
    "    \"TU_2_2_1_2\": 5,\n",
    "    \"TU_2_2_2_1\": 10,\n",
    "    \"TU_2_2_2_2\": 10,\n",
    "    \"TU_2_3_1_1\": 5,\n",
    "    \"TU_2_3_1_2\": 5,\n",
    "    \"TU_2_3_2_1\": 10,\n",
    "    \"TU_2_3_2_2\": 10,\n",
    "    \"TU_3_1_1_1\": 5,\n",
    "    \"TU_3_1_1_2\": 5,\n",
    "    \"TU_3_1_2_1\": 10,\n",
    "    \"TU_3_1_2_2\": 10,\n",
    "    \"TU_3_2_1_1\": 5,\n",
    "    \"TU_3_2_1_2\": 5,\n",
    "    \"TU_3_2_2_1\": 10,\n",
    "    \"TU_3_2_2_2\": 10,\n",
    "    \"TU_3_3_1_1\": 5,\n",
    "    \"TU_3_3_1_2\": 5,\n",
    "    \"TU_3_3_2_1\": 10,\n",
    "    \"TU_3_3_2_2\": 10,\n",
    "    }   \n",
    "\n",
    "# Dictionary to look up potential buyers for a given experiment id. key: experiment id, value: potential buyer\n",
    "TU_buyers_dict = {\n",
    "    \"TU_1_1_1_1\": \"friend\",\n",
    "    \"TU_1_1_1_2\": \"stranger\",\n",
    "    \"TU_1_1_2_1\": \"friend\",\n",
    "    \"TU_1_1_2_2\": \"stranger\",\n",
    "    \"TU_1_2_1_1\": \"friend\",\n",
    "    \"TU_1_2_1_2\": \"stranger\",\n",
    "    \"TU_1_2_2_1\": \"friend\",\n",
    "    \"TU_1_2_2_2\": \"stranger\",\n",
    "    \"TU_1_3_1_1\": \"friend\",\n",
    "    \"TU_1_3_1_2\": \"stranger\",\n",
    "    \"TU_1_3_2_1\": \"friend\",\n",
    "    \"TU_1_3_2_2\": \"stranger\",\n",
    "    \"TU_2_1_1_1\": \"friend\",\n",
    "    \"TU_2_1_1_2\": \"stranger\",\n",
    "    \"TU_2_1_2_1\": \"friend\",\n",
    "    \"TU_2_1_2_2\": \"stranger\",\n",
    "    \"TU_2_2_1_1\": \"friend\",\n",
    "    \"TU_2_2_1_2\": \"stranger\",\n",
    "    \"TU_2_2_2_1\": \"friend\",\n",
    "    \"TU_2_2_2_2\": \"stranger\",\n",
    "    \"TU_2_3_1_1\": \"friend\",\n",
    "    \"TU_2_3_1_2\": \"stranger\",\n",
    "    \"TU_2_3_2_1\": \"friend\",\n",
    "    \"TU_2_3_2_2\": \"stranger\",\n",
    "    \"TU_3_1_1_1\": \"friend\",\n",
    "    \"TU_3_1_1_2\": \"stranger\",\n",
    "    \"TU_3_1_2_1\": \"friend\",\n",
    "    \"TU_3_1_2_2\": \"stranger\",\n",
    "    \"TU_3_2_1_1\": \"friend\",\n",
    "    \"TU_3_2_1_2\": \"stranger\",\n",
    "    \"TU_3_2_2_1\": \"friend\",\n",
    "    \"TU_3_2_2_2\": \"stranger\",\n",
    "    \"TU_3_3_1_1\": \"friend\",\n",
    "    \"TU_3_3_1_2\": \"stranger\",\n",
    "    \"TU_3_3_2_1\": \"friend\",\n",
    "    \"TU_3_3_2_2\": \"stranger\",\n",
    "}\n",
    "\n",
    "# Dictionary to look up original results for a given experiment id. key: experiment id, value: original results\n",
    "TU_results_dict = {\n",
    "    \"TU_1_1_1_1\": [68, 26, 3, 6],\n",
    "    \"TU_1_1_1_2\": [6, 77, 10, 6],\n",
    "    \"TU_1_1_2_1\": [65, 26, 6, 3],\n",
    "    \"TU_1_1_2_2\": [6, 16, 58, 19],\n",
    "    \"TU_1_2_1_1\": [14, 79, 7, 14],\n",
    "    \"TU_1_2_1_2\": [0, 79, 7, 14],\n",
    "    \"TU_1_2_2_1\": [7, 79, 4, 9],\n",
    "    \"TU_1_2_2_2\": [0, 14, 57, 29],\n",
    "    \"TU_1_3_1_1\": [0, 69, 23, 8],\n",
    "    \"TU_1_3_1_2\": [0, 42, 46, 12],\n",
    "    \"TU_1_3_2_1\": [0, 15, 69, 15],\n",
    "    \"TU_1_3_2_2\": [0, 0, 73, 27],\n",
    "    \"TU_2_1_1_1\": [68, 26, 3, 6],\n",
    "    \"TU_2_1_1_2\": [6, 77, 10, 6],\n",
    "    \"TU_2_1_2_1\": [65, 26, 6, 3],\n",
    "    \"TU_2_1_2_2\": [6, 16, 58, 19],\n",
    "    \"TU_2_2_1_1\": [14, 79, 7, 14],\n",
    "    \"TU_2_2_1_2\": [0, 79, 7, 14],\n",
    "    \"TU_2_2_2_1\": [7, 79, 4, 9],\n",
    "    \"TU_2_2_2_2\": [0, 14, 57, 29],\n",
    "    \"TU_2_3_1_1\": [0, 69, 23, 8],\n",
    "    \"TU_2_3_1_2\": [0, 42, 46, 12],\n",
    "    \"TU_2_3_2_1\": [0, 15, 69, 15],\n",
    "    \"TU_2_3_2_2\": [0, 0, 73, 27],\n",
    "    \"TU_3_1_1_1\": [68, 26, 3, 6],\n",
    "    \"TU_3_1_1_2\": [6, 77, 10, 6],\n",
    "    \"TU_3_1_2_1\": [65, 26, 6, 3],\n",
    "    \"TU_3_1_2_2\": [6, 16, 58, 19],\n",
    "    \"TU_3_2_1_1\": [14, 79, 7, 14],\n",
    "    \"TU_3_2_1_2\": [0, 79, 7, 14],\n",
    "    \"TU_3_2_2_1\": [7, 79, 4, 9],\n",
    "    \"TU_3_2_2_2\": [0, 14, 57, 29],\n",
    "    \"TU_3_3_1_1\": [0, 69, 23, 8],\n",
    "    \"TU_3_3_1_2\": [0, 42, 46, 12],\n",
    "    \"TU_3_3_2_1\": [0, 15, 69, 15],\n",
    "    \"TU_3_3_2_2\": [0, 0, 73, 27],\n",
    "}\n",
    "\n",
    "# Dictionary to look configuration given experiment id. key: experiment id, value: configuration (0-12)\n",
    "TU_configurations_dict = {\n",
    "    \"TU_1_1_1_1\": 1,\n",
    "    \"TU_1_1_1_2\": 2,\n",
    "    \"TU_1_1_2_1\": 3,\n",
    "    \"TU_1_1_2_2\": 4,\n",
    "    \"TU_1_2_1_1\": 5,\n",
    "    \"TU_1_2_1_2\": 6,\n",
    "    \"TU_1_2_2_1\": 7,\n",
    "    \"TU_1_2_2_2\": 8,\n",
    "    \"TU_1_3_1_1\": 9,\n",
    "    \"TU_1_3_1_2\": 10,\n",
    "    \"TU_1_3_2_1\": 11,\n",
    "    \"TU_1_3_2_2\": 12,\n",
    "    \"TU_2_1_1_1\": 1,\n",
    "    \"TU_2_1_1_2\": 2,\n",
    "    \"TU_2_1_2_1\": 3,\n",
    "    \"TU_2_1_2_2\": 4,\n",
    "    \"TU_2_2_1_1\": 5,\n",
    "    \"TU_2_2_1_2\": 6,\n",
    "    \"TU_2_2_2_1\": 7,\n",
    "    \"TU_2_2_2_2\": 8,\n",
    "    \"TU_2_3_1_1\": 9,\n",
    "    \"TU_2_3_1_2\": 10,\n",
    "    \"TU_2_3_2_1\": 11,\n",
    "    \"TU_2_3_2_2\": 12,\n",
    "    \"TU_3_1_1_1\": 1,\n",
    "    \"TU_3_1_1_2\": 2,\n",
    "    \"TU_3_1_2_1\": 3,\n",
    "    \"TU_3_1_2_2\": 4,\n",
    "    \"TU_3_2_1_1\": 5,\n",
    "    \"TU_3_2_1_2\": 6,\n",
    "    \"TU_3_2_2_1\": 7,\n",
    "    \"TU_3_2_2_2\": 8,\n",
    "    \"TU_3_3_1_1\": 9,\n",
    "    \"TU_3_3_1_2\": 10,\n",
    "    \"TU_3_3_2_1\": 11,\n",
    "    \"TU_3_3_2_2\": 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up functions to repeatedly prompt the LLMs\n",
    "\n",
    "- Helper function to extract dollar amount of given answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the dollar amount of the answer from LLMs\n",
    "def extract_dollar_amounts(answers):\n",
    "    # Only return values that start with \"$\"\n",
    "    valid_prices = [item for item in answers if item.startswith(\"$\")]\n",
    "    # Delete the \"$\" from the beginning of each price\n",
    "    prices = [item.replace('$', '') for item in valid_prices]\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times for OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU_run_experiment(experiment_id, n, progress_bar, temperature):\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = TU_model_dict[experiment_id], \n",
    "            max_tokens = 2,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer by only giving a single price in dollars and cents without an explanation.\"},        \n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"{TU_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"}\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU_model_dict[experiment_id], TU_initial_costs_dict[experiment_id], TU_orientation_prices_dict[experiment_id],\n",
    "                TU_buyers_dict[experiment_id], answers, n_observations, TU_configurations_dict[experiment_id], TU_results_dict[experiment_id]]\n",
    "\n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard  (returns dataframe right away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU_run_experiment_dashboard(experiment_id, n, progress_bar, temperature):\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = TU_model_dict[experiment_id], \n",
    "            max_tokens = 2,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer by only giving a single price in dollars and cents without an explanation.\"},        \n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"{TU_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"}\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU_model_dict[experiment_id], TU_initial_costs_dict[experiment_id], TU_orientation_prices_dict[experiment_id],\n",
    "                TU_buyers_dict[experiment_id], answers, n_observations, TU_configurations_dict[experiment_id], TU_results_dict[experiment_id]]\n",
    "    # Set index\n",
    "    results = pd.DataFrame(results).set_index(pd.Index(\n",
    "        [\"experiment_id\", \"temperature\", \"model\", \"initial_cost\", \"orientation_price\", \"buyer\", \"answers\", \"Obs.\", \"configuration\", \"original\"]))\n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU_run_experiment_llama(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            TU_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\":  \"Answer by only giving a single price in dollars and cents without an explanation.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 5, \n",
    "                \"prompt\": f\"{TU_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU_model_dict[experiment_id], TU_initial_costs_dict[experiment_id], TU_orientation_prices_dict[experiment_id],\n",
    "               TU_buyers_dict[experiment_id], answers, n_observations, TU_configurations_dict[experiment_id], TU_results_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard (returns dataframe right away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU_run_experiment_llama_dashboard(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            TU_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\":  \"Answer by only giving a single price in dollars and cents without an explanation.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 5, \n",
    "                \"prompt\": f\"{TU_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU_model_dict[experiment_id], TU_initial_costs_dict[experiment_id], TU_orientation_prices_dict[experiment_id],\n",
    "               TU_buyers_dict[experiment_id], answers, n_observations, TU_configurations_dict[experiment_id], TU_results_dict[experiment_id]]\n",
    "    # Set index\n",
    "    results = pd.DataFrame(results).set_index(pd.Index(\n",
    "        [\"experiment_id\", \"temperature\", \"model\", \"initial_cost\", \"orientation_price\", \"buyer\", \"answers\", \"Obs.\", \"configuration\", \"original\"]))\n",
    "    \n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to loop run_experiment() over a list of temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU_temperature_loop(function, experiment_id, temperature_list = [0.5, 1, 1.5], n = 50):\n",
    "    \"\"\"\n",
    "    Function to run an experiment with different temperature values.\n",
    "    \n",
    "    Args:\n",
    "        function (function): Function to be used for querying ChatGPT i.e. run_experiment()\n",
    "        experiment_id (str): ID of th e experiment to be run. Contains info about prompt and model\n",
    "        temperature_list (list): List of temperature values to be looped over\n",
    "        n: Number of requests for each prompt per temperature value\n",
    "        max_tokens: Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results_df: Dataframe with experiment results\n",
    "        probs_df: Dataframe with answer probabilities\n",
    "    \"\"\"    \n",
    "    # Empty list for storing results\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize progress bar -> used as input for run_experiment()\n",
    "    progress_bar = tqdm(range(n*len(temperature_list)))\n",
    "\n",
    "    # Loop over different temperature values, calling the input function n times each (i.e. queriyng ChatGPT n times)\n",
    "    for temperature in temperature_list:\n",
    "        results = function(experiment_id = experiment_id, n = n, temperature = temperature, progress_bar = progress_bar) \n",
    "        results_list.append(results)\n",
    "       \n",
    "\n",
    "    # Horizontally concatenate the results, transpose, and set index\n",
    "    results_df = pd.DataFrame(results_list).transpose().set_index(pd.Index(\n",
    "        [\"experiment_id\", \"temperature\", \"model\", \"initial_cost\", \"orientation_price\", \"buyer\", \"answers\", \"Obs.\", \"configuration\", \"original\"]))\n",
    "  \n",
    "   \n",
    "    # Return some information about the experiment as a check\n",
    "    check = f\"In this run, a total of {n*len(temperature_list)} requests were made using {TU_prompt_ids_dict[experiment_id]}.\"\n",
    "    # Print information about the experiment\n",
    "    print(check)\n",
    " \n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPT-3.5-turbo we make 100 requests per prompt & temperature value\n",
    "# Also, we will focus on lower temperature values to get more consise answers. For higher temperature values, the \n",
    "# answers almost always contain the same information, but come with an unnecessary explanation (e.g. \"I would ask for $10, because that is the price that everyone else is asking for.\")\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:00<02:08,  2.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:12<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "results_1_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_1_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:09<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2 = []\n",
    "results_2_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_1_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:43<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3 = []\n",
    "results_3_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_1_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:15<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4 = []\n",
    "results_4_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_1_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:21<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5 = []\n",
    "results_5_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_2_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:19<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6 = []\n",
    "results_6_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_2_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:13<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7 = []\n",
    "results_7_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_2_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:20<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8 = []\n",
    "results_8_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_2_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:58<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[8].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_9 = []\n",
    "results_9_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_3_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_9.append(results_9_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:04<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[9].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_10 = []\n",
    "results_10_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_3_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_10.append(results_10_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:13<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[10].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_11 = []\n",
    "results_11_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_3_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_11.append(results_11_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:11<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using TU_prompts[11].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_12 = []\n",
    "results_12_1 = TU_temperature_loop(TU_run_experiment, \"TU_1_3_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_12.append(results_12_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: GPT-4-1106-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:07<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_1_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:11<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_1_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_1_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:50<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_1_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:06<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_2_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:56<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_2_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:02<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_2_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:05<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_2_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:06<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[8].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_9_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_3_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_9.append(results_9_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:57<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[9].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_10_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_3_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_10.append(results_10_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:16<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[10].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_11_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_3_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_11.append(results_11_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:57<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[11].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_12_2 = TU_temperature_loop(TU_run_experiment, \"TU_2_3_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_12.append(results_12_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: LLama-2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:08<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_1_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [04:15<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_2_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_1_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:36<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_1_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:47<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_4_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_1_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:20<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_5_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_2_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [04:55<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_6_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_2_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:35<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_7_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_2_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:29<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[7].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_8_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_2_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:55<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[8].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_9_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_3_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_9.append(results_9_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:53<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[9].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_10_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_3_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_10.append(results_10_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:20<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[10].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_11_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_3_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_11.append(results_11_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:27<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using TU_prompts[11].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_12_3 = TU_temperature_loop(TU_run_experiment_llama, \"TU_3_3_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_12.append(results_12_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all results and save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>model</th>\n",
       "      <th>initial_cost</th>\n",
       "      <th>orientation_price</th>\n",
       "      <th>buyer</th>\n",
       "      <th>answers</th>\n",
       "      <th>Obs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU_1_1_1_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>friend</td>\n",
       "      <td>[$5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU_1_1_1_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>friend</td>\n",
       "      <td>[$5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU_1_1_1_1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>friend</td>\n",
       "      <td>[$5, $5, $5, $5, $10, $10, $5, $5, $5, $5, $5,...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU_2_1_1_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>friend</td>\n",
       "      <td>[$0, $0, $0, $0, $0, $0, $0, $0, $0, $0, $0, $...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU_2_1_1_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>friend</td>\n",
       "      <td>[$0, $0, $0, $0, $5, $0, $5, $0, $0, $0, $0, $...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU_2_3_2_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>stranger</td>\n",
       "      <td>[$10, $10, $10, $10, $10, $10, $10, $10, $10, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU_2_3_2_2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>stranger</td>\n",
       "      <td>[$10, $10, $10, $10, $10, $10, $10, $10, $10, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TU_3_3_2_2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>stranger</td>\n",
       "      <td>[$15, $15, $15, $15, $15, $15, $15, $15, $15, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TU_3_3_2_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>stranger</td>\n",
       "      <td>[$15, $15, $15, $15, $15, $15, $15, $15, $15, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TU_3_3_2_2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>stranger</td>\n",
       "      <td>[$15, $15, $15, $15, $15, $15, $15, $15, $15, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id temperature               model initial_cost  \\\n",
       "0     TU_1_1_1_1         0.5       gpt-3.5-turbo            0   \n",
       "1     TU_1_1_1_1         1.0       gpt-3.5-turbo            0   \n",
       "2     TU_1_1_1_1         1.5       gpt-3.5-turbo            0   \n",
       "0     TU_2_1_1_1         0.5  gpt-4-1106-preview            0   \n",
       "1     TU_2_1_1_1         1.0  gpt-4-1106-preview            0   \n",
       "..           ...         ...                 ...          ...   \n",
       "1     TU_2_3_2_2         1.0  gpt-4-1106-preview           10   \n",
       "2     TU_2_3_2_2         1.5  gpt-4-1106-preview           10   \n",
       "0     TU_3_3_2_2         0.5         llama-2-70b           10   \n",
       "1     TU_3_3_2_2         1.0         llama-2-70b           10   \n",
       "2     TU_3_3_2_2         1.5         llama-2-70b           10   \n",
       "\n",
       "   orientation_price     buyer  \\\n",
       "0                  5    friend   \n",
       "1                  5    friend   \n",
       "2                  5    friend   \n",
       "0                  5    friend   \n",
       "1                  5    friend   \n",
       "..               ...       ...   \n",
       "1                 10  stranger   \n",
       "2                 10  stranger   \n",
       "0                 10  stranger   \n",
       "1                 10  stranger   \n",
       "2                 10  stranger   \n",
       "\n",
       "                                              answers Obs.  \n",
       "0   [$5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $...  100  \n",
       "1   [$5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $5, $...  100  \n",
       "2   [$5, $5, $5, $5, $10, $10, $5, $5, $5, $5, $5,...   97  \n",
       "0   [$0, $0, $0, $0, $0, $0, $0, $0, $0, $0, $0, $...   50  \n",
       "1   [$0, $0, $0, $0, $5, $0, $5, $0, $0, $0, $0, $...   50  \n",
       "..                                                ...  ...  \n",
       "1   [$10, $10, $10, $10, $10, $10, $10, $10, $10, ...   50  \n",
       "2   [$10, $10, $10, $10, $10, $10, $10, $10, $10, ...   50  \n",
       "0   [$15, $15, $15, $15, $15, $15, $15, $15, $15, ...   50  \n",
       "1   [$15, $15, $15, $15, $15, $15, $15, $15, $15, ...   50  \n",
       "2   [$15, $15, $15, $15, $15, $15, $15, $15, $15, ...   50  \n",
       "\n",
       "[108 rows x 8 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate results\n",
    "results_1_df = pd.concat(results_1, axis = 1).transpose()\n",
    "results_2_df = pd.concat(results_2, axis = 1).transpose()\n",
    "results_3_df = pd.concat(results_3, axis = 1).transpose()\n",
    "results_4_df = pd.concat(results_4, axis = 1).transpose()\n",
    "results_5_df = pd.concat(results_5, axis = 1).transpose()\n",
    "results_6_df = pd.concat(results_6, axis = 1).transpose()\n",
    "results_7_df = pd.concat(results_7, axis = 1).transpose()\n",
    "results_8_df = pd.concat(results_8, axis = 1).transpose()\n",
    "results_9_df = pd.concat(results_9, axis = 1).transpose()\n",
    "results_10_df = pd.concat(results_10, axis = 1).transpose()\n",
    "results_11_df = pd.concat(results_11, axis = 1).transpose()\n",
    "results_12_df = pd.concat(results_12, axis = 1).transpose()\n",
    "\n",
    "# Concatenate all results\n",
    "TU_results = pd.concat([results_1_df, results_2_df, results_3_df, results_4_df, results_5_df,\n",
    "                         results_6_df, results_7_df, results_8_df, results_9_df, results_10_df, results_11_df, results_12_df], axis = 0)\n",
    "\n",
    "# Rename LLama model\n",
    "TU_results['model'] = TU_results['model'].replace('meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', \n",
    "                                  'llama-2-70b')\n",
    "\n",
    "# Display results\n",
    "TU_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "TU_results.to_csv(\"Output/TU_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
