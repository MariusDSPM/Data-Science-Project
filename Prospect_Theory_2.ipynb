{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prospect Theory 2.0\n",
    "\n",
    "This notebook is a continuation of the previous notebook, where we will research how the concrete numbers of gains/losses in the prompts influence the model's response. \n",
    "\n",
    "In all the scenarios we described, the sum of money both described individuals have as exactly the same at the end of the day.\n",
    "Therefore, we first take a look at how survey replies change, if one individuum is in fact better off money-wise.\n",
    "\n",
    "Secondly, we take a look at how the magnitude of gains/and losses affect the responses. For this, we simply scale every number in a given prompt by the same factor.\n",
    "\n",
    "Since we previously established that, for practical purposes, the minimum temperature value of 0, as well as the maximum value of 2 do not really provide us with insightful results, we will now only focus on the values [0.5, 1, 1.5]. Also, the aspect of priming the models will not be regarded here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import replicate\n",
    "import pickle\n",
    "\n",
    "# Get API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set plots to be displayed in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT2_prompt_1 = \"\"\"Mr. A was given tickets involving the World Series. He won 50$ in one lottery and $25 in the other. \n",
    "          Mr. B was given a ticket to a single, larger World Series lottery. He won $75. Based solely on this information, Who is happier? \n",
    "          A: Mister A\n",
    "          B: Mister B\n",
    "          C: No difference.         \n",
    "          Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT2_prompt_2 = f\"\"\"Mr. A received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $100. \n",
    "         He received a similar letter the same day from his state income tax authority saying he owed $50. There were no other repercussions from either mistake. \n",
    "         Mr. B received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $150. There were no other repercussions from his mistake. \n",
    "         Based solely on this information, who was more upset? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT2_prompt_3 = f\"\"\"Mr. A bought his first New York State lottery ticket and won $100. Also, in a freak accident, he damaged the rug in his apartment and had to pay the landlord $80.\n",
    "         Mr. B bought his first New York State lottery ticket and won $20. Based solely on this information, who is happier? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT2_prompt_4 = f\"\"\"Mr. A's car was damaged in a parking lot. He had to spend $200 to repair the damage. The same day the car was damaged, he won $25 in the office football pool.\n",
    "         Mr. B's car was damaged in a parking lot. He had to spend $175 to repair the damage. Based solely on this information, who is more upset?\n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modifying the monetary values inside the prompts\n",
    "\n",
    "The Prospect Theory value function explains why individuals tend to assess the perceived value of e.g. a sum of multiple gains as larger, than one individual sum of the same amount. Since Large Language Models are trained on human data, including for example customer reviews on sales platforms, they might reflect these patterns. \n",
    "But how do LLMs react, if in the given scenarios, one individual is financially clearly better off than the other? And what if we did not deal with small, even numbers, but rather large and odd ones? \n",
    "Another key concept of prospect theory is decreasing sensitivity. A loss of 50$ subtracted from a total amount of 1000$ will not hurt as much, as if we initially only had 100$, hence losing 50% of our total possession. \n",
    "\n",
    "In order to research these 2 aspects, we adapted our original prompts as follows:\n",
    "\n",
    "For every scenario (1-4) we created:\n",
    "- 2 prompts in which A and B have the same amount of money, but the numbers are odd and larger than before (scaled by Pi * 100 or 42 respectively) \n",
    "- 2 prompts in which A is better off (25$ vs. 50$)\n",
    "- 2 prompts in which B is better off (25$ vs. 50$)\n",
    "\n",
    "In the configurations, in which one individual is better off, we did not simply increase/decrease the same number in the prompt, but rather distributed the changes in gains/losses along the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our results comparable to the original study, we compute original answer probabilities\n",
    "PT2_p_scenario1 = [f\"p(A): {round((56/(56+16+15)*100), 2)}%\", f\"p(B): {round((16/(56+16+15)*100), 2)}%\", f\"p(C): {round((15/(56+16+15)*100), 2)}%\"]\n",
    "PT2_p_scenario2 = [f\"p(A): {round((66/(66+14+7)*100), 2)}%\", f\"p(B): {round((14/(66+14+7)*100), 2)}%\", f\"p(C): {round((7/(66+14+7)*100), 2)}%\"]\n",
    "PT2_p_scenario3 = [f\"p(A): {round((22/(22+61+4)*100), 2)}%\", f\"p(B): {round((61/(22+61+4)*100), 2)}%\", f\"p(C): {round((4/(22+61+4)*100), 2)}%\"]\n",
    "PT2_p_scenario4 = [f\"p(A): {round((19/(19+63+5)*100), 2)}%\", f\"p(B): {round((63/(19+63+5)*100), 2)}%\", f\"p(C): {round((5/(19+63+5)*100), 2)}%\"]\n",
    "\n",
    "# Setting up new monetary values to be used in our prompts, they do not really reflect prices, but will be named as such for simplicity\n",
    "prices_1_og = np.array([50, 25, 75]) # A won 50+25, B won 75\n",
    "prices_2_og = np.array([100, 50, 150]) # A lost 100+50, B lost 150\n",
    "prices_3_og = np.array([100, 80, 20]) # A won 100, lost 80, B won 20\n",
    "prices_4_og = np.array([200, 25, 175]) # A lost 200, won 25, B lost 175\n",
    "\n",
    "# New, rather odd-numbered values, but sum for A&B is the same\n",
    "prices_1_odd = np.round(prices_1_og.copy() * np.pi * 100, 2)\n",
    "prices_2_odd = np.round(prices_2_og.copy() * np.pi * 100, 2)\n",
    "prices_3_odd = np.round(prices_3_og.copy() * np.pi * 100, 2)\n",
    "prices_4_odd = np.round(prices_4_og.copy() * np.pi * 100, 2)\n",
    "\n",
    "prices_1_odd2 = np.round(prices_1_og.copy() * np.pi * 42, 2)\n",
    "prices_2_odd2 = np.round(prices_2_og.copy() * np.pi * 42, 2)\n",
    "prices_3_odd2 = np.round(prices_3_og.copy() * np.pi * 42, 2)\n",
    "prices_4_odd2 = np.round(prices_4_og.copy() * np.pi * 42, 2)\n",
    "\n",
    "# Prices, so that A is always better off (labeled as prices_(original scenario)_(who is better off))\n",
    "# We do not simply always increase the first mentioned gain/decrease the first mentioned loss, but rather try and \"distribute\" the changes \n",
    "# Per prompt, only the same number will be changed, but over all prompts, the changes will be distributed\n",
    "\n",
    "# A always better off by 25$\n",
    "prices_1_a25 = prices_1_og.copy() \n",
    "prices_1_a25[0] += 25\n",
    "prices_2_a25 = prices_2_og.copy()\n",
    "prices_2_a25[1] += -25\n",
    "prices_3_a25 = prices_3_og.copy()\n",
    "prices_3_a25[1] += -25\n",
    "prices_4_a25 = prices_4_og.copy()\n",
    "prices_4_a25[2] += +25\n",
    "\n",
    "# A always better off by 50$\n",
    "prices_1_a50 = prices_1_og.copy()\n",
    "prices_1_a50[0] += 50\n",
    "prices_2_a50 = prices_2_og.copy()\n",
    "prices_2_a50[1] += -50\n",
    "prices_3_a50 = prices_3_og.copy()\n",
    "prices_3_a50[1] += -50\n",
    "prices_4_a50 = prices_4_og.copy()\n",
    "prices_4_a50[2] += +50\n",
    "\n",
    "# B always better off by 25$\n",
    "prices_1_b25 = prices_1_og.copy()\n",
    "prices_1_b25[0] += -25\n",
    "prices_2_b25 = prices_2_og.copy()\n",
    "prices_2_b25[1] += +25\n",
    "prices_3_b25 = prices_3_og.copy()\n",
    "prices_3_b25[1] += 25\n",
    "prices_4_b25 = prices_4_og.copy()\n",
    "prices_4_b25[2] += -25\n",
    "\n",
    "# B always better off by 50$\n",
    "prices_1_b50 = prices_1_og.copy()\n",
    "prices_1_b50[0] += -50\n",
    "prices_2_b50 = prices_2_og.copy()\n",
    "prices_2_b50[1] += +50\n",
    "prices_3_b50 = prices_3_og.copy()\n",
    "prices_3_b50[1] += 50\n",
    "prices_4_b50 = prices_4_og.copy()\n",
    "prices_4_b50[2] += -50\n",
    "\n",
    "# Pretty sure there is an easier way, but this is at least robust and easily controllable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up new prompts with modified numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for scenario 1\n",
    "PT2_prompts_1 = []\n",
    "\n",
    "for prices in [prices_1_odd, prices_1_odd2, prices_1_a25, prices_1_a50, prices_1_b25, prices_1_b50]:\n",
    "    prompt = f\"\"\"Mr. A was given tickets involving the World Series. He won {prices[0]}$ in one lottery and {prices[1]}$ in the other. \n",
    "          Mr. B was given a ticket to a single, larger World Series lottery. He won {prices[2]}$. Based solely on this information, Who is happier? \n",
    "          A: Mister A\n",
    "          B: Mister B\n",
    "          C: No difference.         \n",
    "          Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "    PT2_prompts_1.append(prompt)\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for scenario 2\n",
    "PT2_prompts_2 = []\n",
    "\n",
    "for prices in [prices_2_odd, prices_2_odd2, prices_2_a25, prices_2_a50, prices_2_b25, prices_2_b50]:\n",
    "    prompt = f\"\"\"Mr. A received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed ${prices[0]}. \n",
    "         He received a similar letter the same day from his state income tax authority saying he owed ${prices[1]}. There were no other repercussions from either mistake. \n",
    "         Mr. B received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed ${prices[2]}. There were no other repercussions from his mistake. \n",
    "         Based solely on this information, who was more upset? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "    PT2_prompts_2.append(prompt)\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for scenario 3\n",
    "PT2_prompts_3 = []\n",
    "\n",
    "for prices in [prices_3_odd, prices_3_odd2, prices_3_a25, prices_3_a50, prices_3_b25, prices_3_b50]:\n",
    "    prompt = f\"\"\"Mr. A bought his first New York State lottery ticket and won ${prices[0]}. Also, in a freak accident, he damaged the rug in his apartment and had to pay the landlord ${prices[1]}.\n",
    "         Mr. B bought his first New York State lottery ticket and won ${prices[2]}. Based solely on this information, who is happier? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "    PT2_prompts_3.append(prompt)\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for scenario 4\n",
    "PT2_prompts_4 = []\n",
    "\n",
    "for prices in [prices_4_odd, prices_4_odd2, prices_4_a25, prices_4_a50, prices_4_b25, prices_4_b50]:\n",
    "    prompt = f\"\"\"Mr. A's car was damaged in a parking lot. He had to spend ${prices[0]} to repair the damage. The same day the car was damaged, he won ${prices[1]} in the office football pool.\n",
    "         Mr. B's car was damaged in a parking lot. He had to spend ${prices[2]} to repair the damage. Based solely on this information, who is more upset?\n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "    PT2_prompts_4.append(prompt)\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save prompts for use in dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1\n",
    "with open(\"PT2_prompts_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(PT2_prompts_1, file)\n",
    "\n",
    "# Scenario 2\n",
    "with open(\"PT2_prompts_2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(PT2_prompts_2, file)\n",
    "\n",
    "# Scenario 3\n",
    "with open(\"PT2_prompts_3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(PT2_prompts_3, file)\n",
    "\n",
    "# Scenario 4\n",
    "with open(\"PT2_prompts_4.pkl\", \"wb\") as file:\n",
    "    pickle.dump(PT2_prompts_4, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the notebook, we use experiment IDs, that help us extract prompts and price vectors used for the experiments. We construct them as follows:\n",
    "\n",
    "- scenario_model_configuration\n",
    "\n",
    "The scenarios are, like previously:\n",
    "- 1: Segregation of gains\n",
    "- 2: Integration of losses\n",
    "- 3: Cancellation of losses against larger gains\n",
    "- 4: Segregation of silver linings\n",
    "\n",
    "The model attribute refers to the Language Model we are using, namely:\n",
    "- 1: GPT-3.5-Turbo\n",
    "- 2: GPT-4-1106-Preview\n",
    "- 3: LLama-2-70b\n",
    "\n",
    "Configuration refers to the price vector used in this experiment. We number them as follows:\n",
    "- 1: Odd prices 1 (Original * Pi * 100)\n",
    "- 2: Odd prices 2 (Original * Pi * 42 )\n",
    "- 3: A is better off by 25$\n",
    "- 4: A is better off by 50$\n",
    "- 5: B is better off by 25$\n",
    "- 6: B is better off by 50$\n",
    "\n",
    "Experiment id 2_2_4 therefore reads as: Scenario 2, using GPT-4-1106-Preview, where A is better off by 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Maybe add dict with experiment description \n",
    "\n",
    "# Dictionary to look up prompt for a given experiment id. key: experiment id, value: prompt\n",
    "PT2_experiment_prompts_dict = {\n",
    "    \"PT2_1_1_1\": PT2_prompts_1[0],\n",
    "    \"PT2_1_1_2\": PT2_prompts_1[1],\n",
    "    \"PT2_1_1_3\": PT2_prompts_1[2],\n",
    "    \"PT2_1_1_4\": PT2_prompts_1[3],\n",
    "    \"PT2_1_1_5\": PT2_prompts_1[4],\n",
    "    \"PT2_1_1_6\": PT2_prompts_1[5],\n",
    "    \"PT2_2_1_1\": PT2_prompts_2[0],\n",
    "    \"PT2_2_1_2\": PT2_prompts_2[1],\n",
    "    \"PT2_2_1_3\": PT2_prompts_2[2],\n",
    "    \"PT2_2_1_4\": PT2_prompts_2[3],\n",
    "    \"PT2_2_1_5\": PT2_prompts_2[4],\n",
    "    \"PT2_2_1_6\": PT2_prompts_2[5],\n",
    "    \"PT2_3_1_1\": PT2_prompts_3[0],\n",
    "    \"PT2_3_1_2\": PT2_prompts_3[1],\n",
    "    \"PT2_3_1_3\": PT2_prompts_3[2],\n",
    "    \"PT2_3_1_4\": PT2_prompts_3[3],\n",
    "    \"PT2_3_1_5\": PT2_prompts_3[4],\n",
    "    \"PT2_3_1_6\": PT2_prompts_3[5],\n",
    "    \"PT2_4_1_1\": PT2_prompts_4[0],\n",
    "    \"PT2_4_1_2\": PT2_prompts_4[1],\n",
    "    \"PT2_4_1_3\": PT2_prompts_4[2],\n",
    "    \"PT2_4_1_4\": PT2_prompts_4[3],\n",
    "    \"PT2_4_1_5\": PT2_prompts_4[4],\n",
    "    \"PT2_4_1_6\": PT2_prompts_4[5],\n",
    "    \"PT2_1_2_1\": PT2_prompts_1[0],\n",
    "    \"PT2_1_2_2\": PT2_prompts_1[1],\n",
    "    \"PT2_1_2_3\": PT2_prompts_1[2],\n",
    "    \"PT2_1_2_4\": PT2_prompts_1[3],\n",
    "    \"PT2_1_2_5\": PT2_prompts_1[4],\n",
    "    \"PT2_1_2_6\": PT2_prompts_1[5],\n",
    "    \"PT2_2_2_1\": PT2_prompts_2[0],\n",
    "    \"PT2_2_2_2\": PT2_prompts_2[1],\n",
    "    \"PT2_2_2_3\": PT2_prompts_2[2],\n",
    "    \"PT2_2_2_4\": PT2_prompts_2[3],\n",
    "    \"PT2_2_2_5\": PT2_prompts_2[4],\n",
    "    \"PT2_2_2_6\": PT2_prompts_2[5],\n",
    "    \"PT2_3_2_1\": PT2_prompts_3[0],\n",
    "    \"PT2_3_2_2\": PT2_prompts_3[1],\n",
    "    \"PT2_3_2_3\": PT2_prompts_3[2],\n",
    "    \"PT2_3_2_4\": PT2_prompts_3[3],\n",
    "    \"PT2_3_2_5\": PT2_prompts_3[4],\n",
    "    \"PT2_3_2_6\": PT2_prompts_3[5],\n",
    "    \"PT2_4_2_1\": PT2_prompts_4[0],\n",
    "    \"PT2_4_2_2\": PT2_prompts_4[1],\n",
    "    \"PT2_4_2_3\": PT2_prompts_4[2],\n",
    "    \"PT2_4_2_4\": PT2_prompts_4[3],\n",
    "    \"PT2_4_2_5\": PT2_prompts_4[4],\n",
    "    \"PT2_4_2_6\": PT2_prompts_4[5],\n",
    "    \"PT2_1_3_1\": PT2_prompts_1[0],\n",
    "    \"PT2_1_3_2\": PT2_prompts_1[1],\n",
    "    \"PT2_1_3_3\": PT2_prompts_1[2],\n",
    "    \"PT2_1_3_4\": PT2_prompts_1[3],\n",
    "    \"PT2_1_3_5\": PT2_prompts_1[4],\n",
    "    \"PT2_1_3_6\": PT2_prompts_1[5],\n",
    "    \"PT2_2_3_1\": PT2_prompts_2[0],\n",
    "    \"PT2_2_3_2\": PT2_prompts_2[1],\n",
    "    \"PT2_2_3_3\": PT2_prompts_2[2],\n",
    "    \"PT2_2_3_4\": PT2_prompts_2[3],\n",
    "    \"PT2_2_3_5\": PT2_prompts_2[4],\n",
    "    \"PT2_2_3_6\": PT2_prompts_2[5],\n",
    "    \"PT2_3_3_1\": PT2_prompts_3[0],\n",
    "    \"PT2_3_3_2\": PT2_prompts_3[1],\n",
    "    \"PT2_3_3_3\": PT2_prompts_3[2],\n",
    "    \"PT2_3_3_4\": PT2_prompts_3[3],\n",
    "    \"PT2_3_3_5\": PT2_prompts_3[4],\n",
    "    \"PT2_3_3_6\": PT2_prompts_3[5],\n",
    "    \"PT2_4_3_1\": PT2_prompts_4[0],\n",
    "    \"PT2_4_3_2\": PT2_prompts_4[1],\n",
    "    \"PT2_4_3_3\": PT2_prompts_4[2],\n",
    "    \"PT2_4_3_4\": PT2_prompts_4[3],\n",
    "    \"PT2_4_3_5\": PT2_prompts_4[4],\n",
    "    \"PT2_4_3_6\": PT2_prompts_4[5],\n",
    "}\n",
    "\n",
    "# Dictionary to look up price vector used for experiment id. key: experiment id, value: price vector\n",
    "PT2_prices_dict = {\n",
    "    \"PT2_1_1_1\": prices_1_odd,\n",
    "    \"PT2_1_1_2\": prices_1_odd2,\n",
    "    \"PT2_1_1_3\": prices_1_a25,\n",
    "    \"PT2_1_1_4\": prices_1_a50,\n",
    "    \"PT2_1_1_5\": prices_1_b25,\n",
    "    \"PT2_1_1_6\": prices_1_b50,\n",
    "    \"PT2_2_1_1\": prices_2_odd,\n",
    "    \"PT2_2_1_2\": prices_2_odd2,\n",
    "    \"PT2_2_1_3\": prices_2_a25,\n",
    "    \"PT2_2_1_4\": prices_2_a50,\n",
    "    \"PT2_2_1_5\": prices_2_b25,\n",
    "    \"PT2_2_1_6\": prices_2_b50,\n",
    "    \"PT2_3_1_1\": prices_3_odd,\n",
    "    \"PT2_3_1_2\": prices_3_odd2,\n",
    "    \"PT2_3_1_3\": prices_3_a25,\n",
    "    \"PT2_3_1_4\": prices_3_a50,\n",
    "    \"PT2_3_1_5\": prices_3_b25,\n",
    "    \"PT2_3_1_6\": prices_3_b50,\n",
    "    \"PT2_4_1_1\": prices_4_odd,\n",
    "    \"PT2_4_1_2\": prices_4_odd2,\n",
    "    \"PT2_4_1_3\": prices_4_a25,\n",
    "    \"PT2_4_1_4\": prices_4_a50,\n",
    "    \"PT2_4_1_5\": prices_4_b25,\n",
    "    \"PT2_4_1_6\": prices_4_b50,\n",
    "    \"PT2_1_2_1\": prices_1_odd,\n",
    "    \"PT2_1_2_2\": prices_1_odd2,\n",
    "    \"PT2_1_2_3\": prices_1_a25,\n",
    "    \"PT2_1_2_4\": prices_1_a50,\n",
    "    \"PT2_1_2_5\": prices_1_b25,\n",
    "    \"PT2_1_2_6\": prices_1_b50,\n",
    "    \"PT2_2_2_1\": prices_2_odd,\n",
    "    \"PT2_2_2_2\": prices_2_odd2,\n",
    "    \"PT2_2_2_3\": prices_2_a25,\n",
    "    \"PT2_2_2_4\": prices_2_a50,\n",
    "    \"PT2_2_2_5\": prices_2_b25,\n",
    "    \"PT2_2_2_6\": prices_2_b50,\n",
    "    \"PT2_3_2_1\": prices_3_odd,\n",
    "    \"PT2_3_2_2\": prices_3_odd2,\n",
    "    \"PT2_3_2_3\": prices_3_a25,\n",
    "    \"PT2_3_2_4\": prices_3_a50,\n",
    "    \"PT2_3_2_5\": prices_3_b25,\n",
    "    \"PT2_3_2_6\": prices_3_b50,\n",
    "    \"PT2_4_2_1\": prices_4_odd,\n",
    "    \"PT2_4_2_2\": prices_4_odd2,\n",
    "    \"PT2_4_2_3\": prices_4_a25,\n",
    "    \"PT2_4_2_4\": prices_4_a50,\n",
    "    \"PT2_4_2_5\": prices_4_b25,\n",
    "    \"PT2_4_2_6\": prices_4_b50,\n",
    "    \"PT2_1_3_1\": prices_1_odd,\n",
    "    \"PT2_1_3_2\": prices_1_odd2,\n",
    "    \"PT2_1_3_3\": prices_1_a25,\n",
    "    \"PT2_1_3_4\": prices_1_a50,\n",
    "    \"PT2_1_3_5\": prices_1_b25,\n",
    "    \"PT2_1_3_6\": prices_1_b50,\n",
    "    \"PT2_2_3_1\": prices_2_odd,\n",
    "    \"PT2_2_3_2\": prices_2_odd2,\n",
    "    \"PT2_2_3_3\": prices_2_a25,\n",
    "    \"PT2_2_3_4\": prices_2_a50,\n",
    "    \"PT2_2_3_5\": prices_2_b25,\n",
    "    \"PT2_2_3_6\": prices_2_b50,\n",
    "    \"PT2_3_3_1\": prices_3_odd,\n",
    "    \"PT2_3_3_2\": prices_3_odd2,\n",
    "    \"PT2_3_3_3\": prices_3_a25,\n",
    "    \"PT2_3_3_4\": prices_3_a50,\n",
    "    \"PT2_3_3_5\": prices_3_b25,\n",
    "    \"PT2_3_3_6\": prices_3_b50,\n",
    "    \"PT2_4_3_1\": prices_4_odd,\n",
    "    \"PT2_4_3_2\": prices_4_odd2,\n",
    "    \"PT2_4_3_3\": prices_4_a25,\n",
    "    \"PT2_4_3_4\": prices_4_a50,\n",
    "    \"PT2_4_3_5\": prices_4_b25,\n",
    "    \"PT2_4_3_6\": prices_4_b50,\n",
    "}\n",
    "\n",
    "# Dictionary to look up the original results for a given experiment id. key: experiment id, value: original answer probabilities\n",
    "PT2_results_dict = {\n",
    "    \"PT2_1_1_1\": PT2_p_scenario1,\n",
    "    \"PT2_1_1_2\": PT2_p_scenario1,\n",
    "    \"PT2_1_1_3\": PT2_p_scenario1,\n",
    "    \"PT2_1_1_4\": PT2_p_scenario1,\n",
    "    \"PT2_1_1_5\": PT2_p_scenario1,\n",
    "    \"PT2_1_1_6\": PT2_p_scenario1,\n",
    "    \"PT2_2_1_1\": PT2_p_scenario2,\n",
    "    \"PT2_2_1_2\": PT2_p_scenario2,\n",
    "    \"PT2_2_1_3\": PT2_p_scenario2,\n",
    "    \"PT2_2_1_4\": PT2_p_scenario2,\n",
    "    \"PT2_2_1_5\": PT2_p_scenario2,\n",
    "    \"PT2_2_1_6\": PT2_p_scenario2,\n",
    "    \"PT2_3_1_1\": PT2_p_scenario3,\n",
    "    \"PT2_3_1_2\": PT2_p_scenario3,\n",
    "    \"PT2_3_1_3\": PT2_p_scenario3,\n",
    "    \"PT2_3_1_4\": PT2_p_scenario3,\n",
    "    \"PT2_3_1_5\": PT2_p_scenario3,\n",
    "    \"PT2_3_1_6\": PT2_p_scenario3,\n",
    "    \"PT2_4_1_1\": PT2_p_scenario4,\n",
    "    \"PT2_4_1_2\": PT2_p_scenario4,\n",
    "    \"PT2_4_1_3\": PT2_p_scenario4,\n",
    "    \"PT2_4_1_4\": PT2_p_scenario4,\n",
    "    \"PT2_4_1_5\": PT2_p_scenario4,\n",
    "    \"PT2_4_1_6\": PT2_p_scenario4,\n",
    "    \"PT2_1_2_1\": PT2_p_scenario1,\n",
    "    \"PT2_1_2_2\": PT2_p_scenario1,\n",
    "    \"PT2_1_2_3\": PT2_p_scenario1,\n",
    "    \"PT2_1_2_4\": PT2_p_scenario1,\n",
    "    \"PT2_1_2_5\": PT2_p_scenario1,\n",
    "    \"PT2_1_2_6\": PT2_p_scenario1,\n",
    "    \"PT2_2_2_1\": PT2_p_scenario2,\n",
    "    \"PT2_2_2_2\": PT2_p_scenario2,\n",
    "    \"PT2_2_2_3\": PT2_p_scenario2,\n",
    "    \"PT2_2_2_4\": PT2_p_scenario2,\n",
    "    \"PT2_2_2_5\": PT2_p_scenario2,\n",
    "    \"PT2_2_2_6\": PT2_p_scenario2,\n",
    "    \"PT2_3_2_1\": PT2_p_scenario3,\n",
    "    \"PT2_3_2_2\": PT2_p_scenario3,\n",
    "    \"PT2_3_2_3\": PT2_p_scenario3,\n",
    "    \"PT2_3_2_4\": PT2_p_scenario3,\n",
    "    \"PT2_3_2_5\": PT2_p_scenario3,\n",
    "    \"PT2_3_2_6\": PT2_p_scenario3,\n",
    "    \"PT2_4_2_1\": PT2_p_scenario4,\n",
    "    \"PT2_4_2_2\": PT2_p_scenario4,\n",
    "    \"PT2_4_2_3\": PT2_p_scenario4,\n",
    "    \"PT2_4_2_4\": PT2_p_scenario4,\n",
    "    \"PT2_4_2_5\": PT2_p_scenario4,\n",
    "    \"PT2_4_2_6\": PT2_p_scenario4,\n",
    "    \"PT2_1_3_1\": PT2_p_scenario1,\n",
    "    \"PT2_1_3_2\": PT2_p_scenario1,\n",
    "    \"PT2_1_3_3\": PT2_p_scenario1,\n",
    "    \"PT2_1_3_4\": PT2_p_scenario1,   \n",
    "    \"PT2_1_3_5\": PT2_p_scenario1,\n",
    "    \"PT2_1_3_6\": PT2_p_scenario1,\n",
    "    \"PT2_2_3_1\": PT2_p_scenario2,\n",
    "    \"PT2_2_3_2\": PT2_p_scenario2,\n",
    "    \"PT2_2_3_3\": PT2_p_scenario2,\n",
    "    \"PT2_2_3_4\": PT2_p_scenario2,\n",
    "    \"PT2_2_3_5\": PT2_p_scenario2,\n",
    "    \"PT2_2_3_6\": PT2_p_scenario2,\n",
    "    \"PT2_3_3_1\": PT2_p_scenario3,\n",
    "    \"PT2_3_3_2\": PT2_p_scenario3,\n",
    "    \"PT2_3_3_3\": PT2_p_scenario3,\n",
    "    \"PT2_3_3_4\": PT2_p_scenario3,\n",
    "    \"PT2_3_3_5\": PT2_p_scenario3,\n",
    "    \"PT2_3_3_6\": PT2_p_scenario3,\n",
    "    \"PT2_4_3_1\": PT2_p_scenario4,\n",
    "    \"PT2_4_3_2\": PT2_p_scenario4,\n",
    "    \"PT2_4_3_3\": PT2_p_scenario4,\n",
    "    \"PT2_4_3_4\": PT2_p_scenario4,\n",
    "    \"PT2_4_3_5\": PT2_p_scenario4,\n",
    "    \"PT2_4_3_6\": PT2_p_scenario4,\n",
    "}\n",
    "\n",
    "# Dictionary to look up which model to use for a given experiment id. key: experiment id, value: model name\n",
    "PT2_model_dict = {\n",
    "    \"PT2_1_1_1\": \"gpt-3.5-turbo\",  \n",
    "    \"PT2_1_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_1_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_1_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_1_1_5\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_1_1_6\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_5\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_2_1_6\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_5\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_3_1_6\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_5\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_4_1_6\": \"gpt-3.5-turbo\",\n",
    "    \"PT2_1_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_2_5\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_2_6\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_5\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_2_2_6\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_5\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_3_2_6\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_5\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_4_2_6\": \"gpt-4-1106-preview\",\n",
    "    \"PT2_1_3_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_1_3_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_1_3_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_1_3_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_1_3_5\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_1_3_6\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_5\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_2_3_6\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_5\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_3_3_6\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_5\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"PT2_4_3_6\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
    "    }\n",
    "\n",
    "# Dictionary to look up what prompt was used for a given experiment id. key: experiment id, value: prompt variable name\n",
    "PT2_prompt_ids_dict = {\n",
    "    \"PT2_1_1_1\": \"PT2_prompts_1[0]\",\n",
    "    \"PT2_1_1_2\": \"PT2_prompts_1[1]\",\n",
    "    \"PT2_1_1_3\": \"PT2_prompts_1[2]\",\n",
    "    \"PT2_1_1_4\": \"PT2_prompts_1[3]\",\n",
    "    \"PT2_1_1_5\": \"PT2_prompts_1[4]\",\n",
    "    \"PT2_1_1_6\": \"PT2_prompts_1[5]\",\n",
    "    \"PT2_2_1_1\": \"PT2_prompts_2[0]\",\n",
    "    \"PT2_2_1_2\": \"PT2_prompts_2[1]\",\n",
    "    \"PT2_2_1_3\": \"PT2_prompts_2[2]\",\n",
    "    \"PT2_2_1_4\": \"PT2_prompts_2[3]\",\n",
    "    \"PT2_2_1_5\": \"PT2_prompts_2[4]\",\n",
    "    \"PT2_2_1_6\": \"PT2_prompts_2[5]\",\n",
    "    \"PT2_3_1_1\": \"PT2_prompts_3[0]\",\n",
    "    \"PT2_3_1_2\": \"PT2_prompts_3[1]\",\n",
    "    \"PT2_3_1_3\": \"PT2_prompts_3[2]\",\n",
    "    \"PT2_3_1_4\": \"PT2_prompts_3[3]\",\n",
    "    \"PT2_3_1_5\": \"PT2_prompts_3[4]\",\n",
    "    \"PT2_3_1_6\": \"PT2_prompts_3[5]\",\n",
    "    \"PT2_4_1_1\": \"PT2_prompts_4[0]\",\n",
    "    \"PT2_4_1_2\": \"PT2_prompts_4[1]\",\n",
    "    \"PT2_4_1_3\": \"PT2_prompts_4[2]\",\n",
    "    \"PT2_4_1_4\": \"PT2_prompts_4[3]\",\n",
    "    \"PT2_4_1_5\": \"PT2_prompts_4[4]\",\n",
    "    \"PT2_4_1_6\": \"PT2_prompts_4[5]\",\n",
    "    \"PT2_1_2_1\": \"PT2_prompts_1[0]\",\n",
    "    \"PT2_1_2_2\": \"PT2_prompts_1[1]\",\n",
    "    \"PT2_1_2_3\": \"PT2_prompts_1[2]\",\n",
    "    \"PT2_1_2_4\": \"PT2_prompts_1[3]\",\n",
    "    \"PT2_1_2_5\": \"PT2_prompts_1[4]\",\n",
    "    \"PT2_1_2_6\": \"PT2_prompts_1[5]\",\n",
    "    \"PT2_2_2_1\": \"PT2_prompts_2[0]\",\n",
    "    \"PT2_2_2_2\": \"PT2_prompts_2[1]\",\n",
    "    \"PT2_2_2_3\": \"PT2_prompts_2[2]\",\n",
    "    \"PT2_2_2_4\": \"PT2_prompts_2[3]\",\n",
    "    \"PT2_2_2_5\": \"PT2_prompts_2[4]\",\n",
    "    \"PT2_2_2_6\": \"PT2_prompts_2[5]\",\n",
    "    \"PT2_3_2_1\": \"PT2_prompts_3[0]\",\n",
    "    \"PT2_3_2_2\": \"PT2_prompts_3[1]\",\n",
    "    \"PT2_3_2_3\": \"PT2_prompts_3[2]\",\n",
    "    \"PT2_3_2_4\": \"PT2_prompts_3[3]\",\n",
    "    \"PT2_3_2_5\": \"PT2_prompts_3[4]\",\n",
    "    \"PT2_3_2_6\": \"PT2_prompts_3[5]\",\n",
    "    \"PT2_4_2_1\": \"PT2_prompts_4[0]\",\n",
    "    \"PT2_4_2_2\": \"PT2_prompts_4[1]\",\n",
    "    \"PT2_4_2_3\": \"PT2_prompts_4[2]\",\n",
    "    \"PT2_4_2_4\": \"PT2_prompts_4[3]\",\n",
    "    \"PT2_4_2_5\": \"PT2_prompts_4[4]\",\n",
    "    \"PT2_4_2_6\": \"PT2_prompts_4[5]\",\n",
    "    \"PT2_1_3_1\": \"PT2_prompts_1[0]\",\n",
    "    \"PT2_1_3_2\": \"PT2_prompts_1[1]\",\n",
    "    \"PT2_1_3_3\": \"PT2_prompts_1[2]\",\n",
    "    \"PT2_1_3_4\": \"PT2_prompts_1[3]\",\n",
    "    \"PT2_1_3_5\": \"PT2_prompts_1[4]\",\n",
    "    \"PT2_1_3_6\": \"PT2_prompts_1[5]\",\n",
    "    \"PT2_2_3_1\": \"PT2_prompts_2[0]\",\n",
    "    \"PT2_2_3_2\": \"PT2_prompts_2[1]\",\n",
    "    \"PT2_2_3_3\": \"PT2_prompts_2[2]\",\n",
    "    \"PT2_2_3_4\": \"PT2_prompts_2[3]\",\n",
    "    \"PT2_2_3_5\": \"PT2_prompts_2[4]\",\n",
    "    \"PT2_2_3_6\": \"PT2_prompts_2[5]\",\n",
    "    \"PT2_3_3_1\": \"PT2_prompts_3[0]\",\n",
    "    \"PT2_3_3_2\": \"PT2_prompts_3[1]\",\n",
    "    \"PT2_3_3_3\": \"PT2_prompts_3[2]\",\n",
    "    \"PT2_3_3_4\": \"PT2_prompts_3[3]\",\n",
    "    \"PT2_3_3_5\": \"PT2_prompts_3[4]\",\n",
    "    \"PT2_3_3_6\": \"PT2_prompts_3[5]\",\n",
    "    \"PT2_4_3_1\": \"PT2_prompts_4[0]\",\n",
    "    \"PT2_4_3_2\": \"PT2_prompts_4[1]\",\n",
    "    \"PT2_4_3_3\": \"PT2_prompts_4[2]\",\n",
    "    \"PT2_4_3_4\": \"PT2_prompts_4[3]\",\n",
    "    \"PT2_4_3_5\": \"PT2_prompts_4[4]\",\n",
    "    \"PT2_4_3_6\": \"PT2_prompts_4[5]\",\n",
    "\n",
    "}\n",
    "\n",
    "# Dictionary to look up scenario number for a given experiment id. key: experiment id, value: scenario number\n",
    "PT2_scenario_dict = {\n",
    "    \"PT2_1_1_1\": 1,\n",
    "    \"PT2_1_1_2\": 1,\n",
    "    \"PT2_1_1_3\": 1,\n",
    "    \"PT2_1_1_4\": 1,\n",
    "    \"PT2_1_1_5\": 1,\n",
    "    \"PT2_1_1_6\": 1,\n",
    "    \"PT2_2_1_1\": 2,\n",
    "    \"PT2_2_1_2\": 2,\n",
    "    \"PT2_2_1_3\": 2,\n",
    "    \"PT2_2_1_4\": 2,\n",
    "    \"PT2_2_1_5\": 2,\n",
    "    \"PT2_2_1_6\": 2,\n",
    "    \"PT2_3_1_1\": 3,\n",
    "    \"PT2_3_1_2\": 3,\n",
    "    \"PT2_3_1_3\": 3,\n",
    "    \"PT2_3_1_4\": 3,\n",
    "    \"PT2_3_1_5\": 3,\n",
    "    \"PT2_3_1_6\": 3,\n",
    "    \"PT2_4_1_1\": 4,\n",
    "    \"PT2_4_1_2\": 4,\n",
    "    \"PT2_4_1_3\": 4,\n",
    "    \"PT2_4_1_4\": 4,\n",
    "    \"PT2_4_1_5\": 4,\n",
    "    \"PT2_4_1_6\": 4,\n",
    "    \"PT2_1_2_1\": 1,\n",
    "    \"PT2_1_2_2\": 1,\n",
    "    \"PT2_1_2_3\": 1,\n",
    "    \"PT2_1_2_4\": 1,\n",
    "    \"PT2_1_2_5\": 1,\n",
    "    \"PT2_1_2_6\": 1,\n",
    "    \"PT2_2_2_1\": 2,\n",
    "    \"PT2_2_2_2\": 2, \n",
    "    \"PT2_2_2_3\": 2,\n",
    "    \"PT2_2_2_4\": 2,\n",
    "    \"PT2_2_2_5\": 2,\n",
    "    \"PT2_2_2_6\": 2,\n",
    "    \"PT2_3_2_1\": 3,\n",
    "    \"PT2_3_2_2\": 3,\n",
    "    \"PT2_3_2_3\": 3,\n",
    "    \"PT2_3_2_4\": 3,\n",
    "    \"PT2_3_2_5\": 3,\n",
    "    \"PT2_3_2_6\": 3,\n",
    "    \"PT2_4_2_1\": 4,\n",
    "    \"PT2_4_2_2\": 4,\n",
    "    \"PT2_4_2_3\": 4,\n",
    "    \"PT2_4_2_4\": 4,\n",
    "    \"PT2_4_2_5\": 4,\n",
    "    \"PT2_4_2_6\": 4,\n",
    "    \"PT2_1_3_1\": 1,\n",
    "    \"PT2_1_3_2\": 1,\n",
    "    \"PT2_1_3_3\": 1,\n",
    "    \"PT2_1_3_4\": 1,\n",
    "    \"PT2_1_3_5\": 1,\n",
    "    \"PT2_1_3_6\": 1,\n",
    "    \"PT2_2_3_1\": 2,\n",
    "    \"PT2_2_3_2\": 2,\n",
    "    \"PT2_2_3_3\": 2,\n",
    "    \"PT2_2_3_4\": 2,\n",
    "    \"PT2_2_3_5\": 2,\n",
    "    \"PT2_2_3_6\": 2,\n",
    "    \"PT2_3_3_1\": 3,\n",
    "    \"PT2_3_3_2\": 3,\n",
    "    \"PT2_3_3_3\": 3,\n",
    "    \"PT2_3_3_4\": 3,\n",
    "    \"PT2_3_3_5\": 3,\n",
    "    \"PT2_3_3_6\": 3,\n",
    "    \"PT2_4_3_1\": 4,\n",
    "    \"PT2_4_3_2\": 4,\n",
    "    \"PT2_4_3_3\": 4,\n",
    "    \"PT2_4_3_4\": 4,\n",
    "    \"PT2_4_3_5\": 4,\n",
    "    \"PT2_4_3_6\": 4,\n",
    "}\n",
    "\n",
    "# Dictionary to look up scenario configuration based on experiment id. key: experiment id, value: scenario configuration\n",
    "PT2_configuration_dict = {\n",
    "    \"PT2_1_1_1\": 1,\n",
    "    \"PT2_1_1_2\": 2,\n",
    "    \"PT2_1_1_3\": 3,\n",
    "    \"PT2_1_1_4\": 4,\n",
    "    \"PT2_1_1_5\": 5,\n",
    "    \"PT2_1_1_6\": 6,\n",
    "    \"PT2_2_1_1\": 1,\n",
    "    \"PT2_2_1_2\": 2,\n",
    "    \"PT2_2_1_3\": 3,\n",
    "    \"PT2_2_1_4\": 4,\n",
    "    \"PT2_2_1_5\": 5,\n",
    "    \"PT2_2_1_6\": 6,\n",
    "    \"PT2_3_1_1\": 1,\n",
    "    \"PT2_3_1_2\": 2,\n",
    "    \"PT2_3_1_3\": 3,\n",
    "    \"PT2_3_1_4\": 4,\n",
    "    \"PT2_3_1_5\": 5,\n",
    "    \"PT2_3_1_6\": 6,\n",
    "    \"PT2_4_1_1\": 1,\n",
    "    \"PT2_4_1_2\": 2,\n",
    "    \"PT2_4_1_3\": 3,\n",
    "    \"PT2_4_1_4\": 4,\n",
    "    \"PT2_4_1_5\": 5,\n",
    "    \"PT2_4_1_6\": 6,\n",
    "    \"PT2_1_2_1\": 1,\n",
    "    \"PT2_1_2_2\": 2,\n",
    "    \"PT2_1_2_3\": 3,\n",
    "    \"PT2_1_2_4\": 4,\n",
    "    \"PT2_1_2_5\": 5,\n",
    "    \"PT2_1_2_6\": 6,\n",
    "    \"PT2_2_2_1\": 1,\n",
    "    \"PT2_2_2_2\": 2,\n",
    "    \"PT2_2_2_3\": 3,\n",
    "    \"PT2_2_2_4\": 4,\n",
    "    \"PT2_2_2_5\": 5,\n",
    "    \"PT2_2_2_6\": 6,\n",
    "    \"PT2_3_2_1\": 1,\n",
    "    \"PT2_3_2_2\": 2,\n",
    "    \"PT2_3_2_3\": 3,\n",
    "    \"PT2_3_2_4\": 4,\n",
    "    \"PT2_3_2_5\": 5,\n",
    "    \"PT2_3_2_6\": 6,\n",
    "    \"PT2_4_2_1\": 1,\n",
    "    \"PT2_4_2_2\": 2,\n",
    "    \"PT2_4_2_3\": 3,\n",
    "    \"PT2_4_2_4\": 4,\n",
    "    \"PT2_4_2_5\": 5,\n",
    "    \"PT2_4_2_6\": 6,\n",
    "    \"PT2_1_3_1\": 1,\n",
    "    \"PT2_1_3_2\": 2,\n",
    "    \"PT2_1_3_3\": 3,\n",
    "    \"PT2_1_3_4\": 4,\n",
    "    \"PT2_1_3_5\": 5,\n",
    "    \"PT2_1_3_6\": 6,\n",
    "    \"PT2_2_3_1\": 1,\n",
    "    \"PT2_2_3_2\": 2,\n",
    "    \"PT2_2_3_3\": 3,\n",
    "    \"PT2_2_3_4\": 4,\n",
    "    \"PT2_2_3_5\": 5,\n",
    "    \"PT2_2_3_6\": 6,\n",
    "    \"PT2_3_3_1\": 1,\n",
    "    \"PT2_3_3_2\": 2,\n",
    "    \"PT2_3_3_3\": 3,\n",
    "    \"PT2_3_3_4\": 4,\n",
    "    \"PT2_3_3_5\": 5,\n",
    "    \"PT2_3_3_6\": 6,\n",
    "    \"PT2_4_3_1\": 1,\n",
    "    \"PT2_4_3_2\": 2,\n",
    "    \"PT2_4_3_3\": 3,\n",
    "    \"PT2_4_3_4\": 4,\n",
    "    \"PT2_4_3_5\": 5,\n",
    "    \"PT2_4_3_6\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up functions to repeatedly prompt ChatGPT\n",
    "\n",
    "- Functions to query 1 prompt n times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT2_run_experiment(experiment_id, n, progress_bar, temperature):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT multiple times with a survey having answers designed as: A, B, C.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (str): ID of the experiment to be run. Contains info about prompt and model\n",
    "        n (int): Number of queries to be made\n",
    "        temperature (int): Degree of randomness with range 0 (deterministic) to 2 (random)\n",
    "        max_tokens (int): Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results (list): List containing count of answers for each option, also containing experiment_id, temperature and number of observations\n",
    "        probs (list): List containing probability of each option being chosen, also containing experiment_id, temeperature and number of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = PT2_model_dict[experiment_id], \n",
    "            max_tokens = 1,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Only answer with the letter of the alternative you would choose without any reasoning.\"},        \n",
    "            {\"role\": \"user\", \"content\": PT2_experiment_prompts_dict[experiment_id]},\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\") \n",
    "    B = answers.count(\"B\") \n",
    "    C = answers.count(\"C\") \n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]]\n",
    "\n",
    "    # Getting percentage each answer\n",
    "    p_a = f\"{(A / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "    p_b = f\"{(B / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "    p_c = f\"{(C / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard  (returns dataframe with regular numbers, not percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT2_run_experiment_dashboard(experiment_id, n, progress_bar, temperature):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT multiple times with a survey having answers designed as: A, B, C.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (str): ID of the experiment to be run. Contains info about prompt and model\n",
    "        n (int): Number of queries to be made\n",
    "        temperature (int): Degree of randomness with range 0 (deterministic) to 2 (random)\n",
    "        max_tokens (int): Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results (list): List containing count of answers for each option, also containing experiment_id, temperature and number of observations\n",
    "        probs (list): List containing probability of each option being chosen, also containing experiment_id, temeperature and number of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = PT2_model_dict[experiment_id], \n",
    "            max_tokens = 1,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Only answer with the letter of the alternative you would choose without any reasoning.\"},        \n",
    "            {\"role\": \"user\", \"content\": PT2_experiment_prompts_dict[experiment_id]},\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\") \n",
    "    B = answers.count(\"B\") \n",
    "    C = answers.count(\"C\") \n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = pd.DataFrame([experiment_id, temperature, A, B, C, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]])\n",
    "    results = results.set_index(pd.Index([\"Experiment\", \"Temp\", \"A\", \"B\", \"C\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "\n",
    "\n",
    "    # Getting percentage each answer\n",
    "    p_a = (A / (len_correct + 0.000000001)) * 100\n",
    "    p_b = (B / (len_correct + 0.000000001)) * 100\n",
    "    p_c = (C / (len_correct + 0.000000001)) * 100\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = pd.DataFrame([experiment_id, temperature, p_a, p_b, p_c, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]])\n",
    "    probs = results.set_index(pd.Index([\"Experiment\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to query 1 prompt n times (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT2_run_experiment_llama(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            PT2_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\": \"Only answer with the letter of the alternative you would choose without any reasoning.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 2, \n",
    "                \"prompt\": PT2_experiment_prompts_dict[experiment_id]\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\") \n",
    "    B = answers.count(\"B\") \n",
    "    C = answers.count(\"C\") \n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]]\n",
    "\n",
    "    # Getting percentage each answer\n",
    "    p_a = f\"{(A / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "    p_b = f\"{(B / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "    p_c = f\"{(C / (len_correct + 0.000000001)) * 100:.2f}%\"\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard  (returns dataframe with regular numbers, not percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT2_run_experiment_llama_dashboard(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            PT2_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\": \"Only answer with the letter of the alternative you would choose without any reasoning.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 2, \n",
    "                \"prompt\": PT2_experiment_prompts_dict[experiment_id]\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\") \n",
    "    B = answers.count(\"B\") \n",
    "    C = answers.count(\"C\") \n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = pd.Dataframe([experiment_id, temperature, A, B, C, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]])\n",
    "    results = results.set_index(pd.Index([\"Experiment\", \"Temp\", \"A\", \"B\", \"C\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "\n",
    "    # Getting percentage each answer\n",
    "    p_a = (A / (len_correct + 0.000000001)) * 100\n",
    "    p_b = (B / (len_correct + 0.000000001)) * 100\n",
    "    p_c = (C / (len_correct + 0.000000001)) * 100\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = pd.DataFrame(experiment_id, temperature, p_a, p_b, p_c, len_correct, PT2_model_dict[experiment_id], PT2_scenario_dict[experiment_id], PT2_configuration_dict[experiment_id]])\n",
    "    probs = probs.set_index(pd.Index([\"Experiment\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to loop run_experiment() over a list of temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT2_temperature_loop(function, experiment_id, temperature_list = [0.5, 1, 1.5], n = 50):\n",
    "    \"\"\"\n",
    "    Function to run an experiment with different temperature values.\n",
    "    \n",
    "    Args:\n",
    "        function (function): Function to be used for querying ChatGPT i.e. run_experiment()\n",
    "        experiment_id (str): ID of th e experiment to be run. Contains info about prompt and model\n",
    "        temperature_list (list): List of temperature values to be looped over\n",
    "        n: Number of requests for each prompt per temperature value\n",
    "        max_tokens: Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results_df: Dataframe with experiment results\n",
    "        probs_df: Dataframe with answer probabilities\n",
    "    \"\"\"    \n",
    "    # Empty lists for storing results\n",
    "    results_list = []\n",
    "    probs_list = []\n",
    "    # Initialize progress bar -> used as input for run_experiment()\n",
    "    progress_bar = tqdm(range(n*len(temperature_list)))\n",
    "\n",
    "    # Loop over different temperature values, calling the input function n times each (i.e. queriyng ChatGPT n times)\n",
    "    for temperature in temperature_list:\n",
    "        results, probs = function(experiment_id = experiment_id, n = n, temperature = temperature, progress_bar = progress_bar) \n",
    "        results_list.append(results)\n",
    "        probs_list.append(probs)\n",
    "\n",
    "    # Horizontally concatenate the results, transpose, and set index\n",
    "    results_df = pd.DataFrame(results_list).transpose().set_index(pd.Index([\"Experiment\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "    probs_df = pd.DataFrame(probs_list).transpose().set_index(pd.Index([\"Experiment\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Configuration\"]))\n",
    "   \n",
    "    # Return some information about the experiment as a check\n",
    "    check = f\"In this run, a total of {n*len(temperature_list)} requests were made using {PT2_prompt_ids_dict[experiment_id]}.\"\n",
    "    # Print information about the experiment\n",
    "    print(check)\n",
    "    # Print original results \n",
    "    # print(f\"The original results were {results_dict[experiment_id]}.\")\n",
    "\n",
    "    return results_df, probs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPT-3.5-turbo we make 100 requests per prompt & temperature value\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:50<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:26<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:13<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:09<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:16<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:15<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using PT2_prompts_1[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PT2_probs_scenario1 = []\n",
    "for experiment_id in [\"PT2_1_1_1\", \"PT2_1_1_2\", \"PT2_1_1_3\", \"PT2_1_1_4\", \"PT2_1_1_5\", \"PT2_1_1_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario1.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:25<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:29<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:27<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:19<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:38<00:00,  1.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:32<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_2[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PT2_probs_scenario2 = []\n",
    "for experiment_id in [\"PT2_2_1_1\", \"PT2_2_1_2\", \"PT2_2_1_3\", \"PT2_2_1_4\", \"PT2_2_1_5\", \"PT2_2_1_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario2.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:25<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:27<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:27<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:24<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:31<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:38<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_3[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PT2_probs_scenario3 = []\n",
    "for experiment_id in [\"PT2_3_1_1\", \"PT2_3_1_2\", \"PT2_3_1_3\", \"PT2_3_1_4\", \"PT2_3_1_5\", \"PT2_3_1_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario3.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:32<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:25<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:19<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:27<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:29<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:29<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 300 requests were made using prompts_4[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PT2_probs_scenario4 = []\n",
    "for experiment_id in [\"PT2_4_1_1\", \"PT2_4_1_2\", \"PT2_4_1_3\", \"PT2_4_1_4\", \"PT2_4_1_5\", \"PT2_4_1_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario4.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: GPT-4-1106-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since GPT-4 is a much more expensive model to use, we only make 50 requests per prompt & temperature value\n",
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:26<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:21<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:21<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:12<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:26<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:27<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using PT2_prompts_1[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment_id in [\"PT2_1_2_1\", \"PT2_1_2_2\", \"PT2_1_2_3\", \"PT2_1_2_4\", \"PT2_1_2_5\", \"PT2_1_2_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario1.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:40<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:31<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:38<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:11<00:00,  1.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:41<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:39<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_2[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment_id in [\"PT2_2_2_1\", \"PT2_2_2_2\", \"PT2_2_2_3\", \"PT2_2_2_4\", \"PT2_2_2_5\", \"PT2_2_2_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario2.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:34<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [04:40<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [11:32<00:00,  4.62s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:36<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [21:39<00:00,  8.67s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:34<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_3[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment_id in [\"PT2_3_2_1\", \"PT2_3_2_2\", \"PT2_3_2_3\", \"PT2_3_2_4\", \"PT2_3_2_5\", \"PT2_3_2_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario3.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:48<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:31<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[1].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:32<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[2].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:30<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[3].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:06<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[4].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:40<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this run, a total of 150 requests were made using prompts_4[5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment_id in [\"PT2_4_2_1\", \"PT2_4_2_2\", \"PT2_4_2_3\", \"PT2_4_2_4\", \"PT2_4_2_5\", \"PT2_4_2_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario4.append(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save results to .csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT2_probs_scenario1 = pd.concat(PT2_probs_scenario1, axis = 1).transpose()\n",
    "PT2_probs_scenario2 = pd.concat(PT2_probs_scenario2, axis = 1).transpose()\n",
    "PT2_probs_scenario3 = pd.concat(PT2_probs_scenario3, axis = 1).transpose()\n",
    "PT2_probs_scenario4 = pd.concat(PT2_probs_scenario4, axis = 1).transpose()\n",
    "\n",
    "PT2_probs = pd.concat([PT2_probs_scenario1, PT2_probs_scenario2, PT2_probs_scenario3, PT2_probs_scenario4], axis = 0)\n",
    "#PT2_probs.to_csv(\"Output/PT2_probs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: LLama-2-70b\n",
    "\n",
    "!!! Use max_new_tokens of at least 2, as llama tends to begin answers with a blank space !!!\n",
    "\n",
    "Runs on most expensive hardware option of \n",
    "\n",
    "8x Nvidia A40 (Large) GPU \t$0.005800/sec\n",
    "($20.88/hr) \t8x \t48x \t8x 48GB \t680GB\n",
    "\n",
    "Running the below cells costs approximately 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLama model is rather expensive to use, so we only make 50 requests per prompt & temperature value\n",
    "N = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataframes again to append new results\n",
    "PT2_probs = pd.read_csv(\"Output/PT2_probs.csv\")\n",
    "PT2_probs_scenario1 = PT2_probs[PT2_probs[\"Scenario\"] == 1]\n",
    "PT2_probs_scenario2 = PT2_probs[PT2_probs[\"Scenario\"] == 2]\n",
    "PT2_probs_scenario3 = PT2_probs[PT2_probs[\"Scenario\"] == 3]\n",
    "PT2_probs_scenario4 = PT2_probs[PT2_probs[\"Scenario\"] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_id in [\"PT2_1_3_1\", \"PT2_1_3_2\", \"PT2_1_3_3\", \"PT2_1_3_4\", \"PT2_1_3_5\", \"PT2_1_3_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment_llama, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario1 = pd.concat([PT2_probs_scenario1, probs.transpose()], axis = 0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Temp</th>\n",
       "      <th>p(A)</th>\n",
       "      <th>p(B)</th>\n",
       "      <th>p(C)</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Configuration</th>\n",
       "      <th>Priming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.33%</td>\n",
       "      <td>70.83%</td>\n",
       "      <td>20.83%</td>\n",
       "      <td>48</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.55%</td>\n",
       "      <td>86.36%</td>\n",
       "      <td>9.09%</td>\n",
       "      <td>44</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>60.00%</td>\n",
       "      <td>40.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>2.00%</td>\n",
       "      <td>98.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PT2_2_3_6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50</td>\n",
       "      <td>meta/llama-2-70b-chat:02e509c789964a7ea8736978...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Experiment Temp   p(A)     p(B)     p(C) Obs.  \\\n",
       "0         NaN  PT2_2_3_1  0.5  0.00%  100.00%    0.00%   50   \n",
       "1         NaN  PT2_2_3_1  1.0  0.00%  100.00%    0.00%   50   \n",
       "2         NaN  PT2_2_3_1  1.5  8.33%   70.83%   20.83%   48   \n",
       "0         NaN  PT2_2_3_2  0.5  0.00%  100.00%    0.00%   50   \n",
       "1         NaN  PT2_2_3_2  1.0  0.00%  100.00%    0.00%   50   \n",
       "2         NaN  PT2_2_3_2  1.5  4.55%   86.36%    9.09%   44   \n",
       "0         NaN  PT2_2_3_3  0.5  0.00%    0.00%  100.00%   50   \n",
       "1         NaN  PT2_2_3_3  1.0  0.00%    0.00%  100.00%   50   \n",
       "2         NaN  PT2_2_3_3  1.5  0.00%    0.00%  100.00%   50   \n",
       "0         NaN  PT2_2_3_4  0.5  0.00%  100.00%    0.00%   50   \n",
       "1         NaN  PT2_2_3_4  1.0  0.00%  100.00%    0.00%   50   \n",
       "2         NaN  PT2_2_3_4  1.5  0.00%   60.00%   40.00%   50   \n",
       "0         NaN  PT2_2_3_5  0.5  0.00%    0.00%  100.00%   50   \n",
       "1         NaN  PT2_2_3_5  1.0  0.00%    0.00%  100.00%   50   \n",
       "2         NaN  PT2_2_3_5  1.5  0.00%    2.00%   98.00%   50   \n",
       "0         NaN  PT2_2_3_6  0.5  0.00%    0.00%  100.00%   50   \n",
       "1         NaN  PT2_2_3_6  1.0  0.00%    0.00%  100.00%   50   \n",
       "2         NaN  PT2_2_3_6  1.5  0.00%    0.00%  100.00%   50   \n",
       "\n",
       "                                               Model Scenario Configuration  \\\n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             1   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             1   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             1   \n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             2   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             2   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             2   \n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             3   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             3   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             3   \n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             4   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             4   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             4   \n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             5   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             5   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             5   \n",
       "0  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             6   \n",
       "1  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             6   \n",
       "2  meta/llama-2-70b-chat:02e509c789964a7ea8736978...        2             6   \n",
       "\n",
       "   Priming  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for experiment_id in [\"PT2_2_3_1\", \"PT2_2_3_2\", \"PT2_2_3_3\", \"PT2_2_3_4\", \"PT2_2_3_5\", \"PT2_2_3_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment_llama, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario2 = pd.concat([PT2_probs_scenario2, probs.transpose()], axis = 0)\n",
    "\n",
    "PT2_probs_scenario2[PT2_probs_scenario2[\"Model\"] == \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_id in [\"PT2_3_3_1\", \"PT2_3_3_2\", \"PT2_3_3_3\", \"PT2_3_3_4\", \"PT2_3_3_5\", \"PT2_3_3_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment_llama, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario3 = pd.concat([PT2_probs_scenario3, probs.transpose()], axis = 0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_id in [\"PT2_4_3_1\", \"PT2_4_3_2\", \"PT2_4_3_3\", \"PT2_4_3_4\", \"PT2_4_3_5\", \"PT2_4_3_6\"]:\n",
    "    results, probs = PT2_temperature_loop(PT2_run_experiment_llama, experiment_id, temperature_list = [0.5, 1, 1.5], n = N)\n",
    "    PT2_probs_scenario4 = pd.concat([PT2_probs_scenario4, probs.transpose()], axis = 0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save the results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all data in one df \n",
    "PT2_probs = pd.concat([PT2_probs_scenario1, PT2_probs_scenario2, PT2_probs_scenario3, PT2_probs_scenario4], axis = 0)\n",
    "\n",
    "# Rename LLama model\n",
    "PT2_probs['Model'] = PT2_probs['Model'].replace('meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', \n",
    "                                  'llama-2-70b')\n",
    "\n",
    "# Transform probabilities to float for plotting\n",
    "PT2_probs[\"p(A)\"] = PT2_probs[\"p(A)\"].str.rstrip('%').astype('float')\n",
    "PT2_probs[\"p(B)\"] = PT2_probs[\"p(B)\"].str.rstrip('%').astype('float')\n",
    "PT2_probs[\"p(C)\"] = PT2_probs[\"p(C)\"].str.rstrip('%').astype('float')\n",
    "\n",
    "# Add priming column for uniform layout\n",
    "vector = [0] * len(PT2_probs)\n",
    "PT2_probs[\"Priming\"] = vector\n",
    "\n",
    "# Save to .csv-file\n",
    "PT2_probs.to_csv(\"Output/PT2_probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [30:13<?, ?it/s]\n",
      "  0%|          | 0/150 [29:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Examplatory display\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mPT2_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "# Examplatory display\n",
    "PT2_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
