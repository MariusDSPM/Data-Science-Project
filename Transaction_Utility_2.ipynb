{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Utility Theory 2\n",
    "\n",
    "This notebook aims to recreate some of the empirical findings of Thaler, R. (1985). Mental accounting and consumer choice. Marketing Science, 4(3), 199-214. \n",
    "Specifically we are interested in whether LLMs' responses are similar to the original responses in **section 3** of the paper, regarding the Beer question.\n",
    "\n",
    "However, we will extend this experiment with the notion of income sensitivity. The various income values are inspired by Brand, James and Israeli, Ayelet and Ngwe, Donald, Using GPT for Market Research (March 21, 2023). Harvard Business School Marketing Unit Working Paper No. 23-062, Available at SSRN: https://ssrn.com/abstract=4395751 or http://dx.doi.org/10.2139/ssrn.4395751. Here, they research the willingness to pay for various goods, prompting the LLM with a stated annual income of $50k, $70k, and $120k.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import replicate\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get openAI API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set plots to be displayed in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up prompts for the experiment\n",
    "\n",
    "- LLMs used in the experiment:\n",
    "    - GPT-3.5-Turbo         (ID = 1)\n",
    "    - GPT-4-1106-Preview    (ID = 2)\n",
    "    - LLama-70b             (ID = 3)\n",
    "\n",
    "We can differentiate between the following scenario combinations:\n",
    "\n",
    "- Place of purchase:\n",
    "    - A fancy resort hotel              (ID = 1)\n",
    "    - A small, run-down grocery store   (ID = 2)\n",
    "\n",
    "- Level of annual income:\n",
    "    - No information about income       (ID = 1)\n",
    "    - $50.000                           (ID = 2)\n",
    "    - $70.000                           (ID = 3)\n",
    "    - $120.000                          (ID = 4)\n",
    "\n",
    "Similar to the Prospect Theory and Decoy Effect notebooks, we will use experiment IDs to run the study. The IDs will be constructed as:\n",
    "\n",
    "*TU2_model__placeofpurchase_income*\n",
    "\n",
    "Therefore, TU2_3_2_1 would mean we used LLama-70b, the beer is to be purchased at a fancy resort hotel and no information about the annual income is given in the prompt.\n",
    "\n",
    "**Median answers: $2.65 for hotel and $1.50 for store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of place of purchase\n",
    "place = [\"a fancy resort hotel.\", \"a small, run-down grocery store.\"]\n",
    "\n",
    "# Set up list of annual income levels\n",
    "incomes = [\"\", \"You have an annual income of $50k.\", \"You have an annual income of $70k.\", \"You have an annual income of $120k.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for resort hotel\n",
    "hotel_prompts = []\n",
    "for income in incomes:\n",
    "    prompt = f\"\"\"You are lying on the beach on a hot day. All you have to drink is ice water. For the last hour you have been thinking about how much you would enjoy a nice cold bottle \n",
    "    of beer. A companion gets up to go make a phone call and offers o bring back a beer from the only nearby place where beer is sold, a fancy resort hotel. He says that the beer might\n",
    "    be expensive and so aks how much you are willing to pay for the beer. He says that he will buy the beer if it costs as much or less than the price you state. But if it costs more \n",
    "    than the price you state he will not buy it. You trust your friend, and there is no possibility of bargaining with the bartender. {income} What price do you tell him?\"\"\"\n",
    "    hotel_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for grocery store \n",
    "store_prompts = []\n",
    "for income in incomes:\n",
    "    prompt = f\"\"\"You are lying on the beach on a hot day. All you have to drink is ice water. For the last hour you have been thinking about how much you would enjoy a nice cold bottle \n",
    "    of beer. A companion gets up to go make a phone call and offers o bring back a beer from the only nearby place where beer is sold, a small, run-down grocery store. He says that\n",
    "    the beer might be expensive and so aks how much you are willing to pay for the beer. He says that he will buy the beer if it costs as much or less than the price you state. \n",
    "    But if it costs more than the price you state he will not buy it. You trust your friend, and there is no possibility of bargaining with the store owner. {income} What price do you tell him?\"\"\"\n",
    "    store_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into one list\n",
    "TU2_prompts = hotel_prompts + store_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TU2_prompts[0]: fancy resort hotel, no info about income (Configuration 1)\n",
    "- TU2_prompts[1]: fancy resort hotel, $50k annual income (Configuration 2)\n",
    "- TU2_prompts[2]: fancy resort hotel, $70k annual income (Configuration 3)\n",
    "- TU2_prompts[3]: fancy resort hotel, $120k annual income (Configuration 4)\n",
    "- TU2_prompts[4]: store, no info about income (Configuration 5)\n",
    "- TU2_prompts[5]: store, $50k annual income (Configuration 6)\n",
    "- TU2_prompts[6]: store, $70k annual income (Configuration 7)\n",
    "- TU2_prompts[7]: store, $120k annual income (Configuration 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather experiment ids\n",
    "TU2_experiment_ids = [\"TU2_1_1_1\", \"TU2_1_1_2\", \"TU2_1_1_3\", \"TU2_1_1_4\", \"TU2_1_2_1\", \"TU2_1_2_2\", \"TU2_1_2_3\", \"TU2_1_2_4\",\n",
    "                      \"TU2_2_1_1\", \"TU2_2_1_2\", \"TU2_2_1_3\", \"TU2_2_1_4\", \"TU2_2_2_1\", \"TU2_2_2_2\", \"TU2_2_2_3\", \"TU2_2_2_4\",\n",
    "                      \"TU2_3_1_1\", \"TU2_3_1_2\", \"TU2_3_1_3\", \"TU2_3_1_4\", \"TU2_3_2_1\", \"TU2_3_2_2\", \"TU2_3_2_3\", \"TU2_3_2_4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up instructions the model should abide by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Answer by only giving a single price in dollars and cents without an explanation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries to store and extract information about the various experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to look up prompt for a given experiment id. key: experiment id, value: prompt\n",
    "TU2_experiment_prompts_dict = {\n",
    "    \"TU2_1_1_1\": TU2_prompts[0],\n",
    "    \"TU2_1_1_2\": TU2_prompts[1],\n",
    "    \"TU2_1_1_3\": TU2_prompts[2],\n",
    "    \"TU2_1_1_4\": TU2_prompts[3],\n",
    "    \"TU2_1_2_1\": TU2_prompts[4],\n",
    "    \"TU2_1_2_2\": TU2_prompts[5],\n",
    "    \"TU2_1_2_3\": TU2_prompts[6],\n",
    "    \"TU2_1_2_4\": TU2_prompts[7],\n",
    "    \"TU2_2_1_1\": TU2_prompts[0],\n",
    "    \"TU2_2_1_2\": TU2_prompts[1],\n",
    "    \"TU2_2_1_3\": TU2_prompts[2],\n",
    "    \"TU2_2_1_4\": TU2_prompts[3],\n",
    "    \"TU2_2_2_1\": TU2_prompts[4],\n",
    "    \"TU2_2_2_2\": TU2_prompts[5],\n",
    "    \"TU2_2_2_3\": TU2_prompts[6],\n",
    "    \"TU2_2_2_4\": TU2_prompts[7],\n",
    "    \"TU2_3_1_1\": TU2_prompts[0],\n",
    "    \"TU2_3_1_2\": TU2_prompts[1],\n",
    "    \"TU2_3_1_3\": TU2_prompts[2],\n",
    "    \"TU2_3_1_4\": TU2_prompts[3],\n",
    "    \"TU2_3_2_1\": TU2_prompts[4],\n",
    "    \"TU2_3_2_2\": TU2_prompts[5],\n",
    "    \"TU2_3_2_3\": TU2_prompts[6],\n",
    "    \"TU2_3_2_4\": TU2_prompts[7]\n",
    "}\n",
    "\n",
    "# Dictionary to look up which model to use for a given experiment id. key: experiment id, value: model name\n",
    "TU2_model_dict = {\n",
    "    \"TU2_1_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_1\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_2\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_3\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_1_2_4\": \"gpt-3.5-turbo\",\n",
    "    \"TU2_2_1_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_3\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_1_4\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_2_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"TU2_3_1_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_1_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_1\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_2\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_3\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"TU2_3_2_4\": \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what prompt variable was used in a given experiment. key: experiment id, value: prompt variable\n",
    "TU2_prompt_ids_dict = {\n",
    "    \"TU2_1_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_1_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_1_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_1_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_1_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_1_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_1_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_1_2_4\": \"TU2_prompts[7]\",\n",
    "    \"TU2_2_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_2_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_2_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_2_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_2_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_2_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_2_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_2_2_4\": \"TU2_prompts[7]\",\n",
    "    \"TU2_3_1_1\": \"TU2_prompts[0]\",\n",
    "    \"TU2_3_1_2\": \"TU2_prompts[1]\",\n",
    "    \"TU2_3_1_3\": \"TU2_prompts[2]\",\n",
    "    \"TU2_3_1_4\": \"TU2_prompts[3]\",\n",
    "    \"TU2_3_2_1\": \"TU2_prompts[4]\",\n",
    "    \"TU2_3_2_2\": \"TU2_prompts[5]\",\n",
    "    \"TU2_3_2_3\": \"TU2_prompts[6]\",\n",
    "    \"TU2_3_2_4\": \"TU2_prompts[7]\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what place variable was used in a given experiment. key: experiment id, value: place variable\n",
    "TU2_places_dict = {\n",
    "    \"TU2_1_1_1\": \"hotel\",\n",
    "    \"TU2_1_1_2\": \"hotel\",\n",
    "    \"TU2_1_1_3\": \"hotel\",\n",
    "    \"TU2_1_1_4\": \"hotel\",\n",
    "    \"TU2_1_2_1\": \"grocery\",\n",
    "    \"TU2_1_2_2\": \"grocery\",\n",
    "    \"TU2_1_2_3\": \"grocery\",\n",
    "    \"TU2_1_2_4\": \"grocery\",\n",
    "    \"TU2_2_1_1\": \"hotel\",\n",
    "    \"TU2_2_1_2\": \"hotel\",\n",
    "    \"TU2_2_1_3\": \"hotel\",\n",
    "    \"TU2_2_1_4\": \"hotel\",\n",
    "    \"TU2_2_2_1\": \"grocery\",\n",
    "    \"TU2_2_2_2\": \"grocery\",\n",
    "    \"TU2_2_2_3\": \"grocery\",\n",
    "    \"TU2_2_2_4\": \"grocery\",\n",
    "    \"TU2_3_1_1\": \"hotel\",\n",
    "    \"TU2_3_1_2\": \"hotel\",\n",
    "    \"TU2_3_1_3\": \"hotel\",\n",
    "    \"TU2_3_1_4\": \"hotel\",\n",
    "    \"TU2_3_2_1\": \"grocery\",\n",
    "    \"TU2_3_2_2\": \"grocery\",\n",
    "    \"TU2_3_2_3\": \"grocery\",\n",
    "    \"TU2_3_2_4\": \"grocery\"\n",
    "}   \n",
    "\n",
    "# Dictionary to look up what income variable was used in a given experiment. key: experiment id, value: income variable\n",
    "TU2_income_dict = {\n",
    "    \"TU2_1_1_1\": \"0\",\n",
    "    \"TU2_1_1_2\": \"$50k\",\n",
    "    \"TU2_1_1_3\": \"$70k\",\n",
    "    \"TU2_1_1_4\": \"$120k\",\n",
    "    \"TU2_1_2_1\": \"0\",\n",
    "    \"TU2_1_2_2\": \"$50k\",\n",
    "    \"TU2_1_2_3\": \"$70k\",\n",
    "    \"TU2_1_2_4\": \"$120k\",\n",
    "    \"TU2_2_1_1\": \"0\",\n",
    "    \"TU2_2_1_2\": \"$50k\",\n",
    "    \"TU2_2_1_3\": \"$70k\",\n",
    "    \"TU2_2_1_4\": \"$120k\",\n",
    "    \"TU2_2_2_1\": \"0\",\n",
    "    \"TU2_2_2_2\": \"$50k\",\n",
    "    \"TU2_2_2_3\": \"$70k\",\n",
    "    \"TU2_2_2_4\": \"$120k\",\n",
    "    \"TU2_3_1_1\": \"0\",\n",
    "    \"TU2_3_1_2\": \"$50k\",\n",
    "    \"TU2_3_1_3\": \"$70k\",\n",
    "    \"TU2_3_1_4\": \"$120k\",\n",
    "    \"TU2_3_2_1\": \"0\",\n",
    "    \"TU2_3_2_2\": \"$50k\",\n",
    "    \"TU2_3_2_3\": \"$70k\",\n",
    "    \"TU2_3_2_4\": \"$120k\"\n",
    "}\n",
    "\n",
    "# Dictionary to look up what configuration was used in a given experiment. key: experiment id, value: configuration\n",
    "TU2_configuration_dict = {\n",
    "    \"TU2_1_1_1\": 1,\n",
    "    \"TU2_1_1_2\": 2,\n",
    "    \"TU2_1_1_3\": 3,\n",
    "    \"TU2_1_1_4\": 4,\n",
    "    \"TU2_1_2_1\": 5,\n",
    "    \"TU2_1_2_2\": 6,\n",
    "    \"TU2_1_2_3\": 7,\n",
    "    \"TU2_1_2_4\": 8,\n",
    "    \"TU2_2_1_1\": 1,\n",
    "    \"TU2_2_1_2\": 2,\n",
    "    \"TU2_2_1_3\": 3,\n",
    "    \"TU2_2_1_4\": 4,\n",
    "    \"TU2_2_2_1\": 5,\n",
    "    \"TU2_2_2_2\": 6,\n",
    "    \"TU2_2_2_3\": 7,\n",
    "    \"TU2_2_2_4\": 8,\n",
    "    \"TU2_3_1_1\": 1,\n",
    "    \"TU2_3_1_2\": 2,\n",
    "    \"TU2_3_1_3\": 3,\n",
    "    \"TU2_3_1_4\": 4,\n",
    "    \"TU2_3_2_1\": 5,\n",
    "    \"TU2_3_2_2\": 6,\n",
    "    \"TU2_3_2_3\": 7,\n",
    "    \"TU2_3_2_4\": 8 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up functions to repeatedly prompt the LLMs\n",
    "\n",
    "- Helper function to extract dollar amount of given answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the dollar amount of the answer from LLMs\n",
    "def extract_dollar_amounts(answers):\n",
    "    # Only return values that start with \"$\"\n",
    "    valid_prices = [item for item in answers if item.startswith(\"$\")]\n",
    "    # Delete the \"$\" from the beginning of each price\n",
    "    prices = [item.replace('$', '') for item in valid_prices]\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times for OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment(experiment_id, n, progress_bar, temperature):\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = TU2_model_dict[experiment_id], \n",
    "            max_tokens = 2,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer by only giving a single price in dollars and cents without an explanation.\"},        \n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"}\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], answers, n_observations, TU2_configuration_dict[experiment_id]]\n",
    "\n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions to query 1 prompt n times (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_run_experiment_llama(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            TU2_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\":  \"Answer by only giving a single price in dollars and cents without an explanation.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 10, \n",
    "                \"prompt\": f\"{TU2_experiment_prompts_dict[experiment_id]} Answer by only giving a single price in dollars and cents without an explanation.\"\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    # Extract valid prices from answers\n",
    "    valid_prices = extract_dollar_amounts(answers)\n",
    "\n",
    "    # Compute number of valid answers\n",
    "    n_observations = len(valid_prices)\n",
    "\n",
    "    # Collect results \n",
    "    results = [experiment_id, temperature, TU2_model_dict[experiment_id], TU2_places_dict[experiment_id],\n",
    "                TU2_income_dict[experiment_id], answers, n_observations, TU2_configuration_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to loop run_experiment() over a list of temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TU2_temperature_loop(function, experiment_id, temperature_list = [0.5, 1, 1.5], n = 50):\n",
    "    \"\"\"\n",
    "    Function to run an experiment with different temperature values.\n",
    "    \n",
    "    Args:\n",
    "        function (function): Function to be used for querying ChatGPT i.e. run_experiment()\n",
    "        experiment_id (str): ID of th e experiment to be run. Contains info about prompt and model\n",
    "        temperature_list (list): List of temperature values to be looped over\n",
    "        n: Number of requests for each prompt per temperature value\n",
    "        max_tokens: Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results_df: Dataframe with experiment results\n",
    "        probs_df: Dataframe with answer probabilities\n",
    "    \"\"\"    \n",
    "    # Empty list for storing results\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize progress bar -> used as input for run_experiment()\n",
    "    progress_bar = tqdm(range(n*len(temperature_list)))\n",
    "\n",
    "    # Loop over different temperature values, calling the input function n times each (i.e. queriyng ChatGPT n times)\n",
    "    for temperature in temperature_list:\n",
    "        results = function(experiment_id = experiment_id, n = n, temperature = temperature, progress_bar = progress_bar) \n",
    "        results_list.append(results)\n",
    "       \n",
    "\n",
    "    # Horizontally concatenate the results, transpose, and set index\n",
    "    results_df = pd.DataFrame(results_list).transpose().set_index(pd.Index(\n",
    "        [\"experiment_id\", \"temperature\", \"model\", \"place\", \"income\", \"answers\", \"n_observations\", \"configuration\"]))\n",
    "  \n",
    "   \n",
    "    # Return some information about the experiment as a check\n",
    "    check = f\"In this run, a total of {n*len(temperature_list)} requests were made using {TU2_prompt_ids_dict[experiment_id]}.\"\n",
    "    # Print information about the experiment\n",
    "    print(check)\n",
    " \n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPT-3.5-turbo we make 100 requests per prompt & temperature value\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = []\n",
    "results_1_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = []\n",
    "results_2_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = []\n",
    "results_3_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4 = []\n",
    "results_4_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_1_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_5 = []\n",
    "results_5_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_6 = []\n",
    "results_6_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_7 = []\n",
    "results_7_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_8 = []\n",
    "results_8_1 = TU2_temperature_loop(TU2_run_experiment, \"TU2_1_2_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: GPT-4-1106-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_1_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_5_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_6_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_7_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_8_2 = TU2_temperature_loop(TU2_run_experiment, \"TU2_2_2_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: LLama-2-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_1.append(results_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_2.append(results_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_3.append(results_3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_1_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_4.append(results_4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_5_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_1\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_5.append(results_5_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_6_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_2\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_6.append(results_6_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_7_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_3\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_7.append(results_7_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_8_3 = TU2_temperature_loop(TU2_run_experiment_llama, \"TU2_3_2_4\", temperature_list = [0.5, 1, 1.5], n = N)\n",
    "results_8.append(results_8_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather all results and save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results\n",
    "results_1_df = pd.concat(results_1, axis = 1).transpose()\n",
    "results_2_df = pd.concat(results_2, axis = 1).transpose()\n",
    "results_3_df = pd.concat(results_3, axis = 1).transpose()\n",
    "results_4_df = pd.concat(results_4, axis = 1).transpose()\n",
    "results_5_df = pd.concat(results_5, axis = 1).transpose()\n",
    "results_6_df = pd.concat(results_6, axis = 1).transpose()\n",
    "results_7_df = pd.concat(results_7, axis = 1).transpose()\n",
    "results_8_df = pd.concat(results_8, axis = 1).transpose()\n",
    "results_9_df = pd.concat(results_9, axis = 1).transpose()\n",
    "\n",
    "\n",
    "# Concatenate all results\n",
    "TU2_results = pd.concat([results_1_df, results_2_df, results_3_df, results_4_df, results_5_df,\n",
    "                         results_6_df, results_7_df, results_8_df, results_9_df], axis = 0)\n",
    "\n",
    "# Rename LLama model\n",
    "TU2_results['model'] = TU2_results['model'].replace('meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', \n",
    "                                  'llama-2-70b')\n",
    "\n",
    "# Display results\n",
    "TU2_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
