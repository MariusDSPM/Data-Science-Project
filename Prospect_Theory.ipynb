{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prospect Theory\n",
    "\n",
    "This notebook aims to recreate some of the findings of **Thaler, Richard (1985), “Mental Accounting and Consumer Choice,” Marketing Science, 4 (3), 199–214.** Specifically, we try to see if LLMs like **ChatGPT** abide by some rules of Mental Accounting and Prospect Theory.\n",
    "\n",
    "\n",
    "Maybe change it to was in every prompt? Who WAS happier? (original phrasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original study: \n",
    "\n",
    "### Scenario 1: Segragation of gains\n",
    "- Mr. A was given tickets to lotteries involving the World Series. He won $50 in one lottery and $25 in the other.\n",
    "- Mr. B was given a ticket to a single, larger World Series lottery. He won $75. Who is happier?\n",
    "\n",
    "| Answer option | Frequency |\n",
    "|--------------|-----------|\n",
    "| A            | 56        |\n",
    "| B            | 16        |\n",
    "| No difference | 15      |\n",
    "\n",
    "(empirical results from the 1985 study) -> No segregation of gains for B\n",
    "\n",
    "### Scenario 2: Integration of losses\n",
    "- Mr. A received a letter from the IRS saying that he made a minor arithmetical mistake on his\n",
    "tax return and owed $100. He received a similar letter the same day from his state income tax\n",
    "authority saying he owed $50. There were no other repercussions from either mistake.\n",
    "- Mr. B received a letter from the IRS saying that he made a minor arithmetical mistake on his tax\n",
    "return and owed $150. There were no other repercussions from his mistake. Who was more upset?\n",
    "\n",
    "| Answer option | Frequency |\n",
    "|--------------|-----------|\n",
    "| A            | 66        |\n",
    "| B            | 14        |\n",
    "| No difference | 7      |\n",
    "\n",
    "(empirical results from the 1985 study) -> No integration of losses for B\n",
    "\n",
    "#### Scenario 3: Cancellation of losses against larger gains\n",
    "- Mr. A bought his first New York State lottery ticket and won $100. Also, in a freak accident,\n",
    "he damaged the rug in his apartment and had to pay the landlord $80.\n",
    "- Mr. B bought his first New York State lottery ticket and won $20? Who is happier?\n",
    "\n",
    "| Answer option | Frequency |\n",
    "|--------------|-----------|\n",
    "| A            | 22        |\n",
    "| B            | 61        |\n",
    "| No difference | 4      |\n",
    "\n",
    "(empirical results from the 1985 study) -> No cancellation of losses against larger gains for A\n",
    "\n",
    "\n",
    "#### Scenario 4: Segregation of \"silver linings\"\n",
    "- Mr. A's car was damaged in a parking lot. He had to spend $200 to repair the damage. The\n",
    "same day the car was damaged, he won $25 in the office football pool.\n",
    "- Mr. B's car was damaged in a parking lot. He had to spend $175 to repair the damage.\n",
    "Who was more upset?\n",
    "\n",
    "| Answer option | Frequency |\n",
    "|--------------|-----------|\n",
    "| A            | 19        |\n",
    "| B            | 63        |\n",
    "| No difference | 5      |\n",
    "\n",
    "(empirical results from the 1985 study) -> No segregation of \"silver linings\" for B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import replicate\n",
    "import plotly.graph_objects as go\n",
    "from ast import literal_eval\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get openAI API key (previously saved as environmental variable)\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set global plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set plots to be displayed in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our results comparable to the original study, we compute original answer probabilities\n",
    "PT_p_scenario1 = [np.round((56/(56+16+15)*100), 2), np.round(16/(56+16+15)*100,2), np.round((15/(56+16+15)*100), 2)]\n",
    "PT_p_scenario2 = [np.round((66/(66+14+7)*100), 2), np.round((14/(66+14+7)*100), 2), np.round((7/(66+14+7)*100), 2)]\n",
    "PT_p_scenario3 = [np.round((22/(22+61+4)*100), 2), np.round((61/(22+61+4)*100), 2), np.round((4/(22+61+4)*100), 2)]\n",
    "PT_p_scenario4 = [np.round((19/(19+63+5)*100), 2), np.round((63/(19+63+5)*100), 2), np.round((5/(19+63+5)*100), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p(A)</th>\n",
       "      <th>p(B)</th>\n",
       "      <th>p(C)</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Obs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.37</td>\n",
       "      <td>18.39</td>\n",
       "      <td>17.24</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.86</td>\n",
       "      <td>16.09</td>\n",
       "      <td>8.05</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.29</td>\n",
       "      <td>70.11</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.84</td>\n",
       "      <td>72.41</td>\n",
       "      <td>5.75</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p(A)   p(B)   p(C) Scenario Obs.\n",
       "0  64.37  18.39  17.24        1   87\n",
       "1  75.86  16.09   8.05        2   87\n",
       "2  25.29  70.11   4.60        3   87\n",
       "3  21.84  72.41   5.75        4   87"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with additional information about experiment\n",
    "PT_original_results = pd.DataFrame([PT_p_scenario1, PT_p_scenario2, PT_p_scenario3, PT_p_scenario4], columns=['p(A)', 'p(B)', 'p(C)'])\n",
    "PT_original_results['Scenario'] = ['1', '2', '3', '4']\n",
    "PT_original_results['Obs.'] = ['87', '87', '87', '87']\n",
    "PT_original_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to .csv for use in Dashboard\n",
    "PT_original_results.to_csv(\"Dashboard/src/data/Input/PT_og_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the prompts used for the experiment\n",
    "\n",
    "We now formulate 8 different prompts: \n",
    "\n",
    "The first four prompts all describe different scenarios in which 2 people, Mister A and Mister B, each lose or win money. Most importantly, in every scenario, the monetary value they both lost or gained is the same. However, this is where Prospect Theory comes into play. Some of the gains/losses are separated or integrated and the four marketing implications of prospect theory arise: Segregation of gains, Integration of losses, cancellation of losses against larger gains and the segregation of \"silver linings\".\n",
    "\n",
    "The last four prompts describe the same situations as before. However, we now instruct the model to take the role of a market researcher that knows about the implications of Prospect Theory.\n",
    "\n",
    "(Since prompt formatting matters, for now we will not insert a line break mid-sentence and try to keep a scenario description for A/B in the same line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 1: Segregation of gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_1 = \"\"\"Mr. A was given tickets involving the World Series. He won 50$ in one lottery and $25 in the other. \n",
    "          Mr. B was given a ticket to a single, larger World Series lottery. He won $75. Based solely on this information, Who was happier? \n",
    "          A: Mister A\n",
    "          B: Mister B\n",
    "          C: No difference.         \n",
    "          Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 2: Integration of losses (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_2 = \"\"\"Mr. A received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $100. \n",
    "         He received a similar letter the same day from his state income tax authority saying he owed $50. There were no other repercussions from either mistake. \n",
    "         Mr. B received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $150. There were no other repercussions from his mistake. \n",
    "         Based solely on this information, who was more upset? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 3: Cancellation of losses against larger gains (unprimed) # Add info about same day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_3 = \"\"\"Mr. A bought his first New York State lottery ticket and won $100. Also, in a freak accident, he damaged the rug in his apartment and had to pay the landlord $80.\n",
    "         Mr. B bought his first New York State lottery ticket and won $20. Based solely on this information, who was happier? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 4: Segregation of \"silver linings\" (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_4 = \"\"\"Mr. A's car was damaged in a parking lot. He had to spend $200 to repair the damage. The same day the car was damaged, he won $25 in the office football pool.\n",
    "         Mr. B's car was damaged in a parking lot. He had to spend $175 to repair the damage. Based solely on this information, who was more upset?\n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 5: Segregation of gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_5 = \"\"\"You are a market researcher and focus on Prospect Theory and Mental Accounting. In a survey you are presented the following situation: \n",
    "          Mr. A was given tickets involving the World Series. He won 50$ in one lottery and 25$ in the other. \n",
    "          Mr. B was given a ticket to a single, larger World Series lottery. He won 75$. Based solely on this information, who was happier?\n",
    "          A: Mister A\n",
    "          B: Mister B\n",
    "          C: No difference.\n",
    "          Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 6: Integration of losses (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_6 = \"\"\"You are a market researcher and focus on Prospect Theory and Mental Accounting. In a survey you are presented the following situation:\n",
    "         Mr. A received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $100. \n",
    "         He received a similar letter the same day from his state income tax authority saying he owed $50. There were no other repercussions from either mistake. \n",
    "         Mr. B received a letter from the IRS saying that he made a minor arithmetical mistake on his tax return and owed $150. There were no other repercussions from his mistake. \n",
    "         Based solely on this information, who was more upset? \n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Prompt 7: Cancellation of losses against larger gains (primed) # Add info about same day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_7 = \"\"\"You are a market researcher and focus on Prospect Theory and Mental Accounting. In a survey you are presented the following situation:\n",
    "         Mr. A bought his first New York State lottery ticket and won $100. Also, in a freak accident, he damaged the rug in his apartment and had to pay the landlord $80.\n",
    "         Mr. B bought his first New York State lottery ticket and won $20? Based solely on this information, who was happier?\n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "\n",
    "# Who is happier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 8: Segregation of \"silver linings\" (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompt_8 = \"\"\"You are a market researcher and focus on Prospect Theory and Mental Accounting. In a survey you are presented the following situation:\n",
    "         Mr. A's car was damaged in a parking lot. He had to spend $200 to repair the damage. The same day the car was damaged, he won $25 in the office football pool.\n",
    "         Mr. B's car was damaged in a parking lot. He had to spend $175 to repair the damage. Based solely on this information, who was more upset?\n",
    "         A: Mister A\n",
    "         B: Mister B\n",
    "         C: No difference.\n",
    "         Which option would you choose? Please answer by only giving the letter of the alternative you would choose without any reasoning.\"\"\"\n",
    "# Who is more upset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save prompts to use them in the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_prompts = [PT_prompt_1, PT_prompt_2, PT_prompt_3, PT_prompt_4, PT_prompt_5, PT_prompt_6, PT_prompt_7, PT_prompt_8]\n",
    "with open ('Dashboard/src/data/Input/PT_prompts.pkl', 'wb') as file:\n",
    "    pickle.dump(PT_prompts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helpful dictionaries \n",
    "\n",
    "The experiments we will run in this notebook are very similar in study design, and for same cases, also similar in the results we expect. We therefore need to make sure, that we associate the results with the correct study design. That is why the following dictionaries are implemented to look up e.g. what model was used for an experiment.\n",
    "\n",
    "They will also be used inside the functions that call the API multiple times and output some information about the experiment in order to identify it correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that returns the literal prompt for a given experiment id (used in function call). key: experiment_id, value: prompt\n",
    "PT_experiment_prompts_dict = {\n",
    "    \"PT_1_1\": PT_prompts[0],\n",
    "    \"PT_1_2\": PT_prompts[1],\n",
    "    \"PT_1_3\": PT_prompts[2],\n",
    "    \"PT_1_4\": PT_prompts[3],\n",
    "    \"PT_1_5\": PT_prompts[4],\n",
    "    \"PT_1_6\": PT_prompts[5],\n",
    "    \"PT_1_7\": PT_prompts[6],\n",
    "    \"PT_1_8\": PT_prompts[7],\n",
    "    \"PT_2_1\": PT_prompts[0],\n",
    "    \"PT_2_2\": PT_prompts[1],\n",
    "    \"PT_2_3\": PT_prompts[2],\n",
    "    \"PT_2_4\": PT_prompts[3],\n",
    "    \"PT_2_5\": PT_prompts[4],\n",
    "    \"PT_2_6\": PT_prompts[5],\n",
    "    \"PT_2_7\": PT_prompts[6],\n",
    "    \"PT_2_8\": PT_prompts[7],\n",
    "    \"PT_3_1\": PT_prompts[0],\n",
    "    \"PT_3_2\": PT_prompts[1],\n",
    "    \"PT_3_3\": PT_prompts[2],\n",
    "    \"PT_3_4\": PT_prompts[3],\n",
    "    \"PT_3_5\": PT_prompts[4],\n",
    "    \"PT_3_6\": PT_prompts[5],\n",
    "    \"PT_3_7\": PT_prompts[6],\n",
    "    \"PT_3_8\": PT_prompts[7],\n",
    "}\n",
    "\n",
    "# The following dictionary is only used for a check in the function calls.\n",
    "# It returns the variable name of the prompt that was used in the experiment. key: experiment_id, value: prompt_name\n",
    "PT_prompt_ids_dict = {\n",
    "    \"PT_1_1\": \"PT_prompts[0]\",\n",
    "    \"PT_1_2\": \"PT_prompts[1]\",\n",
    "    \"PT_1_3\": \"PT_prompts[2]\",\n",
    "    \"PT_1_4\": \"PT_prompts[3]\",\n",
    "    \"PT_1_5\": \"PT_prompts[4]\",\n",
    "    \"PT_1_6\": \"PT_prompts[5]\",\n",
    "    \"PT_1_7\": \"PT_prompts[6]\",\n",
    "    \"PT_1_8\": \"PT_prompts[7]\",\n",
    "    \"PT_2_1\": \"PT_prompts[0]\",\n",
    "    \"PT_2_2\": \"PT_prompts[1]\",\n",
    "    \"PT_2_3\": \"PT_prompts[2]\",\n",
    "    \"PT_2_4\": \"PT_prompts[3]\",\n",
    "    \"PT_2_5\": \"PT_prompts[4]\",\n",
    "    \"PT_2_6\": \"PT_prompts[5]\",\n",
    "    \"PT_2_7\": \"PT_prompts[6]\",\n",
    "    \"PT_2_8\": \"PT_prompts[7]\",\n",
    "    \"PT_3_1\": \"PT_prompts[0]\",\n",
    "    \"PT_3_2\": \"PT_prompts[1]\",\n",
    "    \"PT_3_3\": \"PT_prompts[2]\",\n",
    "    \"PT_3_4\": \"PT_prompts[3]\",\n",
    "    \"PT_3_5\": \"PT_prompts[4]\",\n",
    "    \"PT_3_6\": \"PT_prompts[5]\",\n",
    "    \"PT_3_7\": \"PT_prompts[6]\",\n",
    "    \"PT_3_8\": \"PT_prompts[7]\",\n",
    "}\n",
    "\n",
    "# Dictionary to look up which model to use for a given experiment id (used in function call). key: experiment id, value: model name\n",
    "PT_model_dict = {\n",
    "    \"PT_1_1\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_2\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_3\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_4\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_5\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_6\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_7\": \"gpt-3.5-turbo\",\n",
    "    \"PT_1_8\": \"gpt-3.5-turbo\",\n",
    "    \"PT_2_1\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_2\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_3\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_4\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_5\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_6\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_7\": \"gpt-4-1106-preview\",\n",
    "    \"PT_2_8\": \"gpt-4-1106-preview\",\n",
    "    \"PT_3_1\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_2\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_3\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_4\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_5\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_6\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_7\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    \"PT_3_8\": 'meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3',\n",
    "    }\n",
    "\n",
    "# Dictionary to look up the scenario number of a given experiment ID. key: experiment id, value: scenario number\n",
    "PT_scenario_dict = {\n",
    "    \"PT_1_1\": 1,\n",
    "    \"PT_1_2\": 2,\n",
    "    \"PT_1_3\": 3,\n",
    "    \"PT_1_4\": 4,\n",
    "    \"PT_1_5\": 1,\n",
    "    \"PT_1_6\": 2,\n",
    "    \"PT_1_7\": 3,\n",
    "    \"PT_1_8\": 4,\n",
    "    \"PT_2_1\": 1,\n",
    "    \"PT_2_2\": 2,\n",
    "    \"PT_2_3\": 3,\n",
    "    \"PT_2_4\": 4,\n",
    "    \"PT_2_5\": 1,\n",
    "    \"PT_2_6\": 2,\n",
    "    \"PT_2_7\": 3,\n",
    "    \"PT_2_8\": 4,\n",
    "    \"PT_3_1\": 1,\n",
    "    \"PT_3_2\": 2,\n",
    "    \"PT_3_3\": 3,\n",
    "    \"PT_3_4\": 4,\n",
    "    \"PT_3_5\": 1,\n",
    "    \"PT_3_6\": 2,\n",
    "    \"PT_3_7\": 3,\n",
    "    \"PT_3_8\": 4,\n",
    "}   \n",
    "\n",
    "# Dictionary to look up, whether an experiment used a primed or unprimed prompt. key: experiment id, value: 1 if primed, 0 if unprimed\n",
    "PT_priming_dict = {\n",
    "    \"PT_1_1\": 0,\n",
    "    \"PT_1_2\": 0,\n",
    "    \"PT_1_3\": 0,\n",
    "    \"PT_1_4\": 0,\n",
    "    \"PT_1_5\": 1,\n",
    "    \"PT_1_6\": 1,\n",
    "    \"PT_1_7\": 1,\n",
    "    \"PT_1_8\": 1,\n",
    "    \"PT_2_1\": 0,\n",
    "    \"PT_2_2\": 0,\n",
    "    \"PT_2_3\": 0,\n",
    "    \"PT_2_4\": 0,\n",
    "    \"PT_2_5\": 1,\n",
    "    \"PT_2_6\": 1,\n",
    "    \"PT_2_7\": 1,\n",
    "    \"PT_2_8\": 1,\n",
    "    \"PT_3_1\": 0,\n",
    "    \"PT_3_2\": 0,\n",
    "    \"PT_3_3\": 0,\n",
    "    \"PT_3_4\": 0,\n",
    "    \"PT_3_5\": 1,\n",
    "    \"PT_3_6\": 1,\n",
    "    \"PT_3_7\": 1,\n",
    "    \"PT_3_8\": 1,\n",
    "}\n",
    "\n",
    "# Dictionary to look up original results of the Prospect Theory experiments. Key: experiment id, value: original results\n",
    "PT_results_dict = {\n",
    "    \"PT_1_1\": PT_p_scenario1,\n",
    "    \"PT_1_2\": PT_p_scenario2,\n",
    "    \"PT_1_3\": PT_p_scenario3,\n",
    "    \"PT_1_4\": PT_p_scenario4,\n",
    "    \"PT_1_5\": PT_p_scenario1,\n",
    "    \"PT_1_6\": PT_p_scenario2,\n",
    "    \"PT_1_7\": PT_p_scenario3,\n",
    "    \"PT_1_8\": PT_p_scenario4,\n",
    "    \"PT_2_1\": PT_p_scenario1,\n",
    "    \"PT_2_2\": PT_p_scenario2,\n",
    "    \"PT_2_3\": PT_p_scenario3,\n",
    "    \"PT_2_4\": PT_p_scenario4,\n",
    "    \"PT_2_5\": PT_p_scenario1,\n",
    "    \"PT_2_6\": PT_p_scenario2,\n",
    "    \"PT_2_7\": PT_p_scenario3,\n",
    "    \"PT_2_8\": PT_p_scenario4,\n",
    "    \"PT_3_1\": PT_p_scenario1,\n",
    "    \"PT_3_2\": PT_p_scenario2,\n",
    "    \"PT_3_3\": PT_p_scenario3,\n",
    "    \"PT_3_4\": PT_p_scenario4,\n",
    "    \"PT_3_5\": PT_p_scenario1,\n",
    "    \"PT_3_6\": PT_p_scenario2,\n",
    "    \"PT_3_7\": PT_p_scenario3,\n",
    "    \"PT_3_8\": PT_p_scenario4,\n",
    "    }\n",
    "\n",
    "# Dictionary to look up number of original answers. key: experiment id, value: number of original answers\n",
    "PT_answercount_dict = {\n",
    "    \"PT_1_1\": 87,\n",
    "    \"PT_1_2\": 87,\n",
    "    \"PT_1_3\": 87,\n",
    "    \"PT_1_4\": 87,\n",
    "    \"PT_1_5\": 87,\n",
    "    \"PT_1_6\": 87,\n",
    "    \"PT_1_7\": 87,\n",
    "    \"PT_1_8\": 87,\n",
    "    \"PT_2_1\": 87,\n",
    "    \"PT_2_2\": 87,\n",
    "    \"PT_2_3\": 87,\n",
    "    \"PT_2_4\": 87,\n",
    "    \"PT_2_5\": 87,\n",
    "    \"PT_2_6\": 87,\n",
    "    \"PT_2_7\": 87,\n",
    "    \"PT_2_8\": 87,\n",
    "    \"PT_3_1\": 87,\n",
    "    \"PT_3_2\": 87,\n",
    "    \"PT_3_3\": 87,\n",
    "    \"PT_3_4\": 87,\n",
    "    \"PT_3_5\": 87,\n",
    "    \"PT_3_6\": 87,\n",
    "    \"PT_3_7\": 87,\n",
    "    \"PT_3_8\": 87,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and save for use in Dashboard\n",
    "PT_dictionaries = [PT_experiment_prompts_dict, PT_prompt_ids_dict, PT_model_dict, PT_scenario_dict, PT_priming_dict, PT_results_dict, PT_answercount_dict]\n",
    "with open ('Dashboard/src/data/Input/PT_dictionaries.pkl', 'wb') as file:\n",
    "    pickle.dump(PT_dictionaries, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up functions to repeatedly prompt ChatGPT\n",
    "\n",
    "- Functions to query 1 prompt n times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_run_experiment(experiment_id, n, progress_bar, temperature):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT multiple times with a survey having answers designed as: A, B, C.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (str): ID of the experiment to be run. Contains info about prompt and model\n",
    "        n (int): Number of queries to be made\n",
    "        temperature (int): Degree of randomness with range 0 (deterministic) to 2 (random)\n",
    "        max_tokens (int): Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results (list): List containing count of answers for each option, also containing experiment_id, temperature and number of observations\n",
    "        probs (list): List containing probability of each option being chosen, also containing experiment_id, temeperature and number of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = PT_model_dict[experiment_id], \n",
    "            max_tokens = 1,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Only answer with the letter of the alternative you would choose without any reasoning.\"},        \n",
    "            {\"role\": \"user\", \"content\": PT_experiment_prompts_dict[experiment_id]},\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "        # Update progress bar (given from either temperature loop, or set locally)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\")\n",
    "    B = answers.count(\"B\")\n",
    "    C = answers.count(\"C\")\n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "               PT_priming_dict[experiment_id], PT_results_dict[experiment_id], PT_answercount_dict[experiment_id]]\n",
    "\n",
    "    # Getting percentage of each answer\n",
    "    p_a = f\"{(A / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "    p_b = f\"{(B / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "    p_c = f\"{(C / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "             PT_priming_dict[experiment_id], PT_results_dict[experiment_id], PT_answercount_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard  (returns dataframe with regular numbers, not percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_run_experiment_dashboard(experiment_id, n, temperature):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT multiple times with a survey having answers designed as: A, B, C.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (str): ID of the experiment to be run. Contains info about prompt and model\n",
    "        n (int): Number of queries to be made\n",
    "        temperature (int): Degree of randomness with range 0 (deterministic) to 2 (random)\n",
    "        max_tokens (int): Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results (list): List containing count of answers for each option, also containing experiment_id, temperature and number of observations\n",
    "        probs (list): List containing probability of each option being chosen, also containing experiment_id, temeperature and number of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for _ in range(n): \n",
    "        response = client.chat.completions.create(\n",
    "            model = PT_model_dict[experiment_id], \n",
    "            max_tokens = 1,\n",
    "            temperature = temperature, # range is 0 to 2\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Only answer with the letter of the alternative you would choose without any reasoning.\"},        \n",
    "            {\"role\": \"user\", \"content\": PT_experiment_prompts_dict[experiment_id]},\n",
    "                   ])\n",
    "\n",
    "        # Store the answer in the list\n",
    "        answer = response.choices[0].message.content\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\")\n",
    "    B = answers.count(\"B\")\n",
    "    C = answers.count(\"C\")\n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "               PT_priming_dict[experiment_id], f\"{PT_results_dict[experiment_id]}\", PT_answercount_dict[experiment_id]]\n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.set_index(pd.Index([\"Experiment_id\", \"Temp\", \"A\", \"B\", \"C\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "    results = results.transpose()\n",
    "\n",
    "    # Getting percentage of each answer\n",
    "    p_a = (A / len_correct) * 100 if len_correct != 0 else 0\n",
    "    p_b = (B / len_correct) * 100 if len_correct != 0 else 0\n",
    "    p_c = (C / len_correct) * 100 if len_correct != 0 else 0\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "             PT_priming_dict[experiment_id], f\"{PT_results_dict[experiment_id]}\", PT_answercount_dict[experiment_id]]\n",
    "    probs = pd.DataFrame(probs)\n",
    "    probs = probs.set_index(pd.Index([\"Experiment_id\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "    probs = probs.transpose()\n",
    "        \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to query 1 prompt n times (LLama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_run_experiment_llama(experiment_id, n, progress_bar, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            PT_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\": \"Only answer with the letter of the alternative you would choose without any reasoning.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 2, \n",
    "                \"prompt\": PT_experiment_prompts_dict[experiment_id]\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\") \n",
    "    B = answers.count(\"B\") \n",
    "    C = answers.count(\"C\") \n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "               PT_priming_dict[experiment_id], PT_results_dict[experiment_id], PT_answercount_dict[experiment_id]]\n",
    "\n",
    "    # Getting percentage each answer\n",
    "    p_a = f\"{(A / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "    p_b = f\"{(B / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "    p_c = f\"{(C / len_correct) * 100 if len_correct != 0 else 0:.2f}%\"\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "             PT_priming_dict[experiment_id], PT_results_dict[experiment_id], PT_answercount_dict[experiment_id]]\n",
    "    \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted function for dashboard  (returns dataframe with regular numbers, not percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_run_experiment_llama_dashboard(experiment_id, n, temperature):\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        response = replicate.run(\n",
    "            PT_model_dict[experiment_id],\n",
    "            input = {\n",
    "                \"system_prompt\": \"Only answer with the letter of the alternative you would choose without any reasoning.\",\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 2, \n",
    "                \"prompt\": PT_experiment_prompts_dict[experiment_id]\n",
    "            }\n",
    "        )\n",
    "        # Grab answer and append to list\n",
    "        answer = \"\" # Set to empty string, otherwise it would append the previous answer to the new one\n",
    "        for item in response:\n",
    "            answer = answer + item\n",
    "        answers.append(answer.strip())\n",
    "\n",
    "\n",
    "    # Counting results\n",
    "    A = answers.count(\"A\")\n",
    "    B = answers.count(\"B\")\n",
    "    C = answers.count(\"C\")\n",
    "\n",
    "    # Count of \"correct\" answers, sums over indicator function thack checks if answer is either A, B or C\n",
    "    len_correct = sum(1 for ans in answers if ans in [\"A\", \"B\", \"C\"])\n",
    "    \n",
    "    # Collecting results in a list\n",
    "    results = [experiment_id, temperature, A, B, C, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "               PT_priming_dict[experiment_id], f\"{PT_results_dict[experiment_id]}\", PT_answercount_dict[experiment_id]]\n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.set_index(pd.Index([\"Experiment_id\", \"Temp\", \"A\", \"B\", \"C\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "    results = results.transpose()\n",
    "\n",
    "\n",
    "    # Getting percentage of each answer\n",
    "    p_a = (A / len_correct) * 100 if len_correct != 0 else 0\n",
    "    p_b = (B / len_correct) * 100 if len_correct != 0 else 0\n",
    "    p_c = (C / len_correct) * 100 if len_correct != 0 else 0\n",
    "\n",
    "    # Collect probabilities in a dataframe\n",
    "    probs = [experiment_id, temperature, p_a, p_b, p_c, len_correct, PT_model_dict[experiment_id], PT_scenario_dict[experiment_id],\n",
    "             PT_priming_dict[experiment_id], f\"{PT_results_dict[experiment_id]}\", PT_answercount_dict[experiment_id]]\n",
    "    probs = pd.DataFrame(probs)\n",
    "    probs = probs.set_index(pd.Index([\"Experiment_id\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "    probs = probs.transpose()\n",
    "        \n",
    "    # Give out results\n",
    "    return results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to loop run_experiment() over a list of temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_temperature_loop(function, experiment_id, temperature_list = [0, 0.5, 1, 1.5, 2], n = 50):\n",
    "    \"\"\"\n",
    "    Function to run an experiment with different temperature values.\n",
    "    \n",
    "    Args:\n",
    "        function (function): Function to be used for querying ChatGPT i.e. run_experiment()\n",
    "        experiment_id (str): ID of th e experiment to be run. Contains info about prompt and model\n",
    "        temperature_list (list): List of temperature values to be looped over\n",
    "        n: Number of requests for each prompt per temperature value\n",
    "        max_tokens: Maximum number of tokens in response object\n",
    "        \n",
    "    Returns:\n",
    "        results_df: Dataframe with experiment results\n",
    "        probs_df: Dataframe with answer probabilities\n",
    "    \"\"\"    \n",
    "    # Empty lists for storing results\n",
    "    results_list = []\n",
    "    probs_list = []\n",
    "    # Initialize progress bar -> used as input for run_experiment()\n",
    "    progress_bar = tqdm(range(n*len(temperature_list)))\n",
    "\n",
    "    # Loop over different temperature values, calling the input function n times each (i.e. queriyng ChatGPT n times)\n",
    "    for temperature in temperature_list:\n",
    "        results, probs = function(experiment_id = experiment_id, n = n, temperature = temperature, progress_bar = progress_bar) \n",
    "        results_list.append(results)\n",
    "        probs_list.append(probs)\n",
    "\n",
    "    # Horizontally concatenate the results, transpose, and set index\n",
    "    results_df = pd.DataFrame(results_list).transpose().set_index(pd.Index([\"Experiment_id\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "    probs_df = pd.DataFrame(probs_list).transpose().set_index(pd.Index([\"Experiment_id\", \"Temp\", \"p(A)\", \"p(B)\", \"p(C)\", \"Obs.\", \"Model\", \"Scenario\", \"Priming\", \"Original\", \"Original_count\"]))\n",
    "   \n",
    "\n",
    "    # Print information about the experiment\n",
    "    print(f\"In this run, a total of {n*len(temperature_list)} requests were made using {PT_prompt_ids_dict[experiment_id]}.\")\n",
    "\n",
    "    return results_df, probs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different LLMs\n",
    "\n",
    "The results variables will be structured as: results_model-id_prompt-id.\n",
    "\n",
    "We will refer to \"GPT-3.5-turbo\" as model 1 and \"GPT-4-1106-preview\" as model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: GPT-3.5-Turbo (Model training ended in September 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 1: Segregation of gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of requests per temperature value\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_1, probs_1_1 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_1\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 2: Integration of losses (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_2, probs_1_2 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_2\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 3: Cancellation of losses against larger gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_3, probs_1_3 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_3\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 4: Segregation of \"silver linings\" (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_4, probs_1_4 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_4\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 5: Segregation of gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_5, probs_1_5 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_5\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 6: Integration of losses (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_6, probs_1_6 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_6\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Prompt 7: Cancellation of losses against larger gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_7, probs_1_7 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_7\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 8: Segregation of \"silver linings\" (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_8, probs_1_8 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_1_8\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: GPT-4-1106-preview (Model training ended in April 2023)\n",
    "\n",
    "Since prompting GPT4 is much more expensive, we will only use 50 requests per temperature value instead of 100, as we did for GPT3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of requests per temperature value\n",
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 1: Segregation of gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_1, probs_2_1 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_1\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 2: Integration of losses (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_2, probs_2_2 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_2\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 3: Cancellation of losses against larger gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_3, probs_2_3 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_3\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 4: Segregation of \"silver linings\" (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_4, probs_2_4 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_4\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 5: Segregation of gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_5, probs_2_5 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_5\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 6: Integration of losses (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_6, probs_2_6 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_6\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Prompt 7: Cancellation of losses against larger gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_7, probs_2_7 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_7\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 8: Segregation of \"silver linings\" (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_8, probs_2_8 = PT_temperature_loop(PT_run_experiment, experiment_id = \"PT_2_8\", n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: LLama-2-70b\n",
    "\n",
    "!!! Use max_new_tokens of at least 2, as llama tends to begin answers with a blank space !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_list = [0.01, 0.5, 1, 1.5, 2] # LLama wont take 0 as temperature and has max temperature of 5\n",
    "N = 50 # number of requests per temperature value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 1: Segregation of gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_1, probs_3_1 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_1\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 2: Integration of losses (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_2, probs_3_2 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_2\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 3: Cancellation of losses against larger gains (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_3, probs_3_3 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_3\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 4: Segregation of silver linings (unprimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_4, probs_3_4 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_4\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 5: Segregation of gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_5, probs_3_5 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_5\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 6: Integration of losses (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_6, probs_3_6 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_6\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 7: Cancellation of losses against larger gains (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_7, probs_3_7 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_7\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt 8: Segregation of silver linings (primed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_8, probs_3_8 = PT_temperature_loop(PT_run_experiment_llama, experiment_id = \"PT_3_8\", temperature_list = [0.01, 0.5, 1, 1.5, 2], n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all results\n",
    "PT_probs = pd.concat([probs_1_1, probs_1_2, probs_1_3, probs_1_4, probs_1_5, probs_1_6, probs_1_7, probs_1_8,\n",
    "                      probs_2_1, probs_2_2, probs_2_3, probs_2_4, probs_2_5, probs_2_6, probs_2_7, probs_2_8,\n",
    "                      probs_3_1, probs_3_2, probs_3_3, probs_3_4, probs_3_5, probs_3_6, probs_3_7, probs_3_8], axis = 1).transpose()\n",
    "\n",
    "# Rename llama model\n",
    "# Rename LLama model\n",
    "PT_probs['Model'] = PT_probs['Model'].replace('meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3', \n",
    "                                  'llama-2-70b')\n",
    "\n",
    "# Transform probabilities to float for plotting\n",
    "# We could have not set them up as percentages in the first place, but this is a quick fix\n",
    "PT_probs[\"p(A)\"] = PT_probs[\"p(A)\"].str.rstrip('%').astype('float')\n",
    "PT_probs[\"p(B)\"] = PT_probs[\"p(B)\"].str.rstrip('%').astype('float')\n",
    "PT_probs[\"p(C)\"] = PT_probs[\"p(C)\"].str.rstrip('%').astype('float')\n",
    "\n",
    "# Save to csv \n",
    "PT_probs.to_csv(\"Dashboard/src/data/Output/PT_probs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_id</th>\n",
       "      <th>Temp</th>\n",
       "      <th>p(A)</th>\n",
       "      <th>p(B)</th>\n",
       "      <th>p(C)</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Model</th>\n",
       "      <th>Priming</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Original</th>\n",
       "      <th>Original_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PT_2_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[64.37, 18.39, 17.24]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PT_2_1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[64.37, 18.39, 17.24]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT_2_1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[64.37, 18.39, 17.24]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PT_2_1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[64.37, 18.39, 17.24]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PT_2_1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[64.37, 18.39, 17.24]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>PT_3_8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[21.84, 72.41, 5.75]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>PT_3_8</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[21.84, 72.41, 5.75]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>PT_3_8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[21.84, 72.41, 5.75]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>PT_3_8</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[21.84, 72.41, 5.75]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>PT_3_8</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.78</td>\n",
       "      <td>42.22</td>\n",
       "      <td>45</td>\n",
       "      <td>llama-2-70b</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[21.84, 72.41, 5.75]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Experiment_id  Temp  p(A)   p(B)    p(C)  Obs.               Model  \\\n",
       "0          PT_2_1  0.00   0.0   0.00  100.00    50  gpt-4-1106-preview   \n",
       "1          PT_2_1  0.50   0.0   0.00  100.00    50  gpt-4-1106-preview   \n",
       "2          PT_2_1  1.00   0.0   0.00  100.00    50  gpt-4-1106-preview   \n",
       "3          PT_2_1  1.50   2.0   0.00   98.00    50  gpt-4-1106-preview   \n",
       "4          PT_2_1  2.00   6.0   2.00   92.00    50  gpt-4-1106-preview   \n",
       "..            ...   ...   ...    ...     ...   ...                 ...   \n",
       "115        PT_3_8  0.01   0.0   0.00  100.00    50         llama-2-70b   \n",
       "116        PT_3_8  0.50   0.0   0.00  100.00    50         llama-2-70b   \n",
       "117        PT_3_8  1.00   0.0   0.00  100.00    50         llama-2-70b   \n",
       "118        PT_3_8  1.50   0.0   0.00  100.00    50         llama-2-70b   \n",
       "119        PT_3_8  2.00   0.0  57.78   42.22    45         llama-2-70b   \n",
       "\n",
       "     Priming  Scenario               Original  Original_count  \n",
       "0          0         1  [64.37, 18.39, 17.24]              87  \n",
       "1          0         1  [64.37, 18.39, 17.24]              87  \n",
       "2          0         1  [64.37, 18.39, 17.24]              87  \n",
       "3          0         1  [64.37, 18.39, 17.24]              87  \n",
       "4          0         1  [64.37, 18.39, 17.24]              87  \n",
       "..       ...       ...                    ...             ...  \n",
       "115        1         4   [21.84, 72.41, 5.75]              87  \n",
       "116        1         4   [21.84, 72.41, 5.75]              87  \n",
       "117        1         4   [21.84, 72.41, 5.75]              87  \n",
       "118        1         4   [21.84, 72.41, 5.75]              87  \n",
       "119        1         4   [21.84, 72.41, 5.75]              87  \n",
       "\n",
       "[120 rows x 11 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT_probs = pd.read_csv(\"Dashboard/src/data/Output/PT_probs.csv\")\n",
    "PT_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to plot model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_plot_results(df):\n",
    "\n",
    "    # Transpose for plotting\n",
    "    df = df.transpose()  \n",
    "    # Get language model name\n",
    "    model = df.loc[\"Model\"].iloc[0]\n",
    "    # Get temperature value\n",
    "    temperature = df.loc[\"Temp\"].iloc[0]\n",
    "    # Get number of observations per temperature value\n",
    "    n_observations = df.loc[\"Obs.\"].iloc[0]\n",
    "    # Get original answer probabilities\n",
    "    og_answers = df.loc[\"Original\"].apply(literal_eval).iloc[0]\n",
    "    # Get number of original answers\n",
    "    n_original = df.loc[\"Original_count\"].iloc[0]\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            name = \"Model answers\",\n",
    "            x = [\"p(A)\", \"p(B)\", \"p(C)\"],\n",
    "            y = [df.loc[\"p(A)\"].iloc[0], df.loc[\"p(B)\"].iloc[0], df.loc[\"p(C)\"].iloc[0]],\n",
    "            customdata = [n_observations, n_observations, n_observations], \n",
    "            hovertemplate = \"Percentage: %{y:.2f}%<br>Number of observations: %{customdata}<extra></extra>\",\n",
    "            marker_color = \"rgb(55, 83, 109)\"\n",
    "        ),\n",
    "        go.Bar(\n",
    "            name = \"Original answers\",\n",
    "            x = [\"p(A)\",\"p(B)\", \"p(C)\"],\n",
    "            y = [og_answers[0], og_answers[1], og_answers[2]],\n",
    "            customdata = [n_original, n_original, n_original],\n",
    "            hovertemplate = \"Percentage: %{y:.2f}%<br>Number of observations: %{customdata}<extra></extra>\",\n",
    "            marker_color = \"rgb(26, 118, 255)\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(\n",
    "    barmode = 'group',\n",
    "    xaxis = dict(\n",
    "        title = \"Answer options\",  \n",
    "        title_font=dict(size=18),  \n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title=\"Probability (%)\",  \n",
    "        title_font=dict(size=18), \n",
    "    ),\n",
    "    title = dict(\n",
    "        text=f\"Distribution of answers for temperature {temperature}, using model {model}\",\n",
    "        x = 0.5, # Center alignment horizontally\n",
    "        y = 0.87,  # Vertical alignment\n",
    "        font=dict(size=22),  \n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=1.01,  \n",
    "        y=0.9,\n",
    "        font=dict(family='Arial', size=12, color='black'),\n",
    "        bordercolor='black',  \n",
    "        borderwidth=2,  \n",
    "    ),\n",
    "    bargap = 0.3  # Gap between temperature values\n",
    ")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to plot original results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PT_plot_og_results(df):\n",
    "    n_original = df[\"Obs.\"]  # number of answer options \n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "                name = \"p(A)\",\n",
    "                x = [0.1, 0.3, 0.5, 0.7],\n",
    "                y = [df[\"p(A)\"][0], df[\"p(A)\"][1], df[\"p(A)\"][2], df[\"p(A)\"][3]],\n",
    "                customdata = n_original,\n",
    "                hovertemplate = \"Percentage: %{y:.2f}%<br>Number of observations: %{customdata}<extra></extra>\",\n",
    "                marker_color=\"black\",\n",
    "            ),\n",
    "        go.Bar(\n",
    "                name = \"p(B)\",\n",
    "                x = [0.15, 0.35, 0.55, 0.75],\n",
    "                y = [df[\"p(B)\"][0], df[\"p(B)\"][1], df[\"p(B)\"][2], df[\"p(B)\"][3]],\n",
    "                customdata = n_original,\n",
    "                hovertemplate = \"Percentage: %{y:.2f}%<br>Number of observations: %{customdata}<extra></extra>\",\n",
    "                marker_color=\"rgb(55, 83, 109)\",\n",
    "\n",
    "            ),\n",
    "        go.Bar(\n",
    "                name = \"p(C)\",\n",
    "                x = [0.2, 0.4, 0.6, 0.8],\n",
    "                y = [df[\"p(C)\"][0], df[\"p(C)\"][1], df[\"p(C)\"][2], df[\"p(C)\"][3]],\n",
    "                customdata = n_original,\n",
    "                hovertemplate = \"Percentage: %{y:.2f}%<br>Number of observations: %{customdata}<extra></extra>\",\n",
    "                marker_color=\"rgb(26, 118, 255)\",\n",
    "        )\n",
    "    ])\n",
    "  \n",
    "\n",
    "    fig.update_layout(\n",
    "    barmode = 'group',\n",
    "    xaxis = dict(\n",
    "        title = \"Scenarios\",  \n",
    "        title_font=dict(size=18),\n",
    "        tickfont=dict(size=16),  \n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title=\"Probability (%)\",  \n",
    "        title_font=dict(size=18), \n",
    "    ),\n",
    "    title = dict(\n",
    "        text=f\"Distribution of original answers per scenario\",\n",
    "        x = 0.5, \n",
    "        y = 0.87,  \n",
    "        font=dict(size=22),  \n",
    "    ),\n",
    "    width = 1000,\n",
    "    margin=dict(t=100),\n",
    "    legend=dict(\n",
    "        x=1.01,  \n",
    "        y=0.9,\n",
    "        font=dict(family='Arial', size=12, color='black'),\n",
    "        bordercolor='black', \n",
    "        borderwidth=2,  \n",
    "    ),\n",
    "    \n",
    ")\n",
    "    # Adjust x-axis labels to show 30+ to symbolize aggregation\n",
    "    fig.update_xaxes(\n",
    "    tickvals =[0.15, 0.35, 0.55, 0.75],\n",
    "    ticktext=[\"Scenario 1\", \"Scenario 2\", \"Scenario 3\", \"Scenario 4\"],\n",
    ")\n",
    "    return fig \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_plot_og_results(PT_original_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
