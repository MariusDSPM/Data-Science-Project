# Import required libraries 
import dash
import dash_bootstrap_components as dbc
from dash import Input, Output, dcc, html, State


dash.register_page(__name__, path='/', name='Home', location='above-experiments')


# Start Page
layout = [html.H1("Do Large Language Models behave like a Human?", className="page-heading"),
          html.Hr(),
          dcc.Markdown("""
                       Large Language Models (LLMs) are a type of Artificial Intelligence, able to generate text in real time.     
                       They have been trained on an enormous amount of text data generated by humans, allowing them to not only learn mere facts, but also
                       learn the underlying structure and logic of human language.    
                       In their very nature, they are designed to respond to a prompt with the most likely continuation of the text. 

                       Since the text data used to train these models has been written by humans, one might expect the output of these models to be 
                       human-like.   
                       This would mean, that specific patterns, biases, preferences or even irrational behavior, as exhibited by humans, might 
                       also be reflected in the answers the LLMs generate. Examples, that are especially interesting in our context, would be: 
                       Extremeness Aversion, Loss Aversion, Anchoring Biases or numerous other behavioral phenomena.   
                        
                       Furthermore, because the training data, among other things, contains text that is freely available on the internet, parts of it 
                       are also: Product reviews, forum posts, blog articles, or other forms in which consumers might express their opinions or preferences
                       regarding the products they buy or use.   
                       Therefore, we can expect the answers generated by the models to also contain the underlying reasoning and preferences of the
                       consumers from the training data.   

                       This suggests, that LLMs could be used to simulate human behavior in numerous applications. 
                       One possible application is the field of market research.   
                       Traditional market research methods, such as surveys, conjoint analysis or focus group discussions can be very expensive
                       and take time. With the use of LLMs, it might be possible to gain meaningful insights about a certain product, service or planned
                       marketing measure by simply asking a language model.  
                       If the results obtained by this approach are actually comparable to the results obtained by traditional market research methods,
                       the use of LLMs in this field could hold huge potential for future applications.   

                       To research whether Large Language models actually reflect typical patterns in consumer behavior, we conducted a series of
                       experiments, which each deal with one specific behavioral phenomenon exhibited by humans. In each experiment the original, 
                       human generated, results will be compared to the answers obtained by the LLMs. 
                       """),
          html.Hr(),
          dcc.Markdown("""
                       **The following pages are available:**      

                      You can use the navigation menu to regard the results of the conducted experiments, send individual queries to LLMs via a messaging interface
                      and even conduct your own experiments.    
                       * [Experiments](/overview) - Explore the results of the conducted experiments 
                       * [Chatbot](/chat-bot) - Interact with LLMs to see how they behave (*API keys required*)
                       * [Live Experiment](/live-experiment) - Conduct your own experiment and search for similarities between the behavior of LLMs and humans (*API keys required*)
                       * [Experiment Recreation](/experiment-recreation) - Recreate the conducted experiments yourself (*API keys required*)    
                       
                      On each experiment page, you can find a detailed description of the original experiment or phenomenon, the motivation for conducting the experiment, 
                       as well as some information about the implementation. Each experiments' results are also visualized on the respective page.    
                      The experiment overview page also provides some further insights into some rather technical aspects in this context of research. 
                       """               
            ),
dbc.Accordion(
    [
    dbc.AccordionItem(
    dcc.Markdown(["""
                 Below you can find a list of relevant literature, which we used in preparation for the experiments. This field of research is still relatively new, however
                  there are numerous papers and articles available, that each regard the underlying behavior of Large Language Models from different perspectives.

                  * Brand, James and Israeli, Ayelet and Ngwe, Donald, Using GPT for Market Research (March 21, 2023). Harvard Business School Marketing Unit Working Paper No. 23-062, Available at SSRN: https://ssrn.com/abstract=4395751 or http://dx.doi.org/10.2139/ssrn.4395751       
                  * Dominguez-Olmedo, Ricardo et al. “Questioning the Survey Responses of Large Language Models.” ArXiv abs/2306.07951 (2023): n. pag.     
                  * Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using large language models to simulate multiple humans and replicate human subject studies. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 17, 337–371.    
                  * Hagendorff, Thilo et al. “Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT.” Nature computational science vol. 3,10 (2023): 833-838. doi:10.1038/s43588-023-00527-x    
                  * John J. Horton, 2023. "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?," NBER Working Papers 31122, National Bureau of Economic Research, Inc.       

                 """]), title = "Further readings"),
    ],        
    start_collapsed=True,
),            
          ]
